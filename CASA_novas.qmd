---
bibliography: CASA_novas_bib.bib
csl: harvard-cite-them-right.csl
title: CASA_novas's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto Flex
    monofont: Liberation Mono
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| echo: false
#| results: hide
#| results: hide

#Explanation: Import libraries
import os
import pandas as pd
import requests

#Explanation: Download bib files
def download_bib(url, out_path):
    if not os.path.exists(out_path):
        try:
            response = requests.get(url)
            response.raise_for_status()
            with open(out_path, 'w', encoding='utf-8') as file:
                file.write(response.text)
        except requests.exceptions.RequestException as e:
            print(f"Failed to get document: {e}")

download_bib("https://raw.githubusercontent.com/hanxiang270/CASA_novas/refs/heads/main/CASA_novas_bib.bib", "CASA_novas_bib.bib")

download_bib("https://raw.githubusercontent.com/hanxiang270/CASA_novas/refs/heads/main/harvard-cite-them-right.csl", "harvard-cite-them-right.csl")
```

```{python}
#| echo: false

#Explanation: This code chunk downloads a file from a given URL and saves it locally to a specified destination, caching it if it doesn't already exist or is too small, and returns the local file path.

from urllib.parse import urlparse
import requests  

def cache_data(src: str, dest: str) -> str:
    """
    Downloads and caches a remote file locally.

    Parameters
    ----------
    src : str
        The remote *source* for the file, any valid URL should work.
    dest : str
        The *destination* location to save the downloaded file.
        
    Returns
    -------
    str
        A string representing the local location of the file.
    """
    url = urlparse(src)
    fn = os.path.split(url.path)[-1]
    dfn = os.path.join(dest, fn)

    if not os.path.isfile(dfn) or os.stat(dfn).st_size < 250:
        print(f"{dfn} not found, downloading!")

        # Create directories if they don't exist
        os.makedirs(dest, exist_ok=True)

        try:
            response = requests.get(src)
            response.raise_for_status()  

            with open(dfn, "wb") as file:
                file.write(response.content)

            print("\tDone downloading...")

            f_size = os.stat(dfn).st_size
            print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

        except requests.exceptions.RequestException as e:
            print(f"Failed to download file from {src}")
            raise e

    else:
        print(f"Found {dfn} locally!")
        f_size = os.stat(dfn).st_size
        print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

    return dfn
```

```{python}
#| echo: false

#Explanation: The code constructs a URL to access a specific dataset from Inside Airbnb, downloads and caches the file, reads it into a Pandas DataFrame, and then prints the shape and detailed information.

host = 'https://data.insideairbnb.com'
country = 'united-kingdom'
state = 'england'
city = 'london'
ymd = '2023-12-10'
url = f'{host}/{country}/{state}/{city}/{ymd}/data/listings.csv.gz'

listings_raw = pd.read_csv(cache_data(url, os.path.join('data', 'raw')))

print(f"Data frame is {listings_raw.shape[0]:,} x {listings_raw.shape[1]}")
listings_raw.info(verbose=True)
```

```{python}
#| echo: false

#Explanation: The code saves the dataset to a specified folder (Data/Raw) in the current working directory as a CSV file.

path = os.path.join('Data', 'Raw') 
fn = url.split('/')[-1] 
print(f"Writing to: {fn}")

if not os.path.exists(path):
    print(f"Creating {path} under {os.getcwd()}")
    os.makedirs(path)

if not os.path.exists(os.path.join(path, fn)): 
    listings_raw.to_csv(os.path.join(path, fn), index=False) 
    print("Done.")
```

## 1. Who collected the InsideAirbnb data?

::: {.duedate}

( 2 points; Answer due Week 7 )

The Inside Airbnb data was primarily collected by [Murray Cox](https://twitter.com/murrayscox), a Brooklyn based independent researcher who started the project in 2015. While Cox leads the initiative, a network of volunteers, activists, and researchers supports the data collection by providing local context, refining scraping tools, and organizing the raw data.

:::


## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}

( 4 points; Answer due Week 7 )

Inside Airbnb collected data to reveal the impact of short-term lets (STL) on residential communities, often obscured by Airbnb’s PR-driven data. In 2015, Airbnb purged over 1,000 listings from multi-listing hosts before presenting a manipulated snapshot under its ‘transparency initiative’ [@cox_how_2020]. If this scandal had not been exposed, Airbnb’s harm to local housing markets would have been downplayed. As a grassroots project, Inside Airbnb challenges dominant corporate narratives by providing transparent data that highlights Airbnb’s role in gentrification, eviction, and the housing crisis. This empowers researchers, policymakers, and marginalized communities to collectively advocate for fair STL regulations and social justice — exemplifying 'data for co-liberation' [@dignazio_data_2020].
:::


## 3. How did they collect it?

::: {.duedate}

( 5 points; Answer due Week 8 )

:::

## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

( 11 points; Answer due Week 9 )

:::

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}

( 18 points; Answer due {{< var assess.group-date >}} )

It’s important to consider that while the InsideAirbnb data provides insights on the effects of short-term rentals, its use raises several ethical concerns, particularly around privacy, consent, rights to use the data, and bias. 

About privacy and consent, we can mention that although the data is scraped from publicly available information on the Airbnb website, researchers using the data do not seek explicit consent from the individuals whose data is included. Even though hosts have accepted Airbnb’s Terms of Service to make their data public to offer their listings, they often are not able to understand the implications of their consent effectively [@solove_privacy_2012] and they didn’t give consent for other purposes like research [@scassa_ownership_2019]. Moreover, because of the “aggregation effect” [@solove_privacy_2012], combining seemingly simple data, represents risks of exposing sensitive information or personally identifying data, for example when combining photos from the listings with locations and personal details exposed on the reviews. 

On rights to use the data, we found that Airbnb's Terms of Service prohibit the downloading or scraping of data from its site, but this obligation only binds the parties to the agreement and not third parties who use the data [@scassa_ownership_2019]. This raises questions about who owns and controls the data and whether researchers should be allowed to use it without explicit permission. 

Finally, bias is an ethical issue to consider when using the data for research purposes and evidence-based policymaking. While the InsideAirbnb data may appear unbiased, the way it is analysed and interpreted can introduce significant biases emerging from the diverse perspectives and methodologies of the institutions conducting the analysis, that may also result on a reductionist way of assessing the phenomena. For example, [@Quattrone:2018] mentions that municipalities often approach Airbnb regulation through a “false dichotomy,” either allowing the platform to operate without restrictions or banning it entirely. Such reductionist approaches can disproportionately harm small-scale hosts who are not misusing the platform and rely on it to cover living costs, which according to [Airbnb News (2002)](https://news.airbnb.com/en-uk/third-of-hosts-use-airbnb-income-to-afford-rising-living-costs/), might represent up to a third of UK hosts.  

To avoid ethical issues arising from biased analysis, according to D’Ignazio [@dignazio_data_2020], it is important acknowledging that data is not raw, and analysis is not fully neutral and requires transparency about context and methods. Also, it’s necessary to keep in mind that behind numbers on the data there are people whose livelihoods may be directly impacted by policy decisions, so quantitative analysis is not enough to address this kind of phenomenon and must be complemented with qualitative research, consent from those whose data is used and a critical approach of the data. 
:::

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 

::: {.duedate}

( 15 points; Answer due {{< var assess.group-date >}} )

```{python}
#| echo: false

#Explanation: Import packages
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sms
import seaborn as sns
from scipy.stats import chisquare
```

```{python}
#| echo: false

#Explanation: To check the data type for each column by a for loop.
for col in listings_raw.columns:
    print(f"{col}: {listings_raw[col].dtype}")
```

```{python}
#| echo: false

#Explanation: Randomly selects 5 rows from the DataFrame listings_raw using a fixed random seed for reproducibility.

listings_raw.sample(10, random_state=42)

#Observation on columns:
#(1)Some columns contain predominantly null values, such as 'license' and 'description'.
#(2)The 'property_type' and 'room_type' column should be in categorical format, not object.
#(3)Some columns, such as 'price', should be in numeric format, not object.
#To ensure the accuracy and reliability of our analysis, columns with predominantly null values will be dropped. Some 'object' columns will be converted to categorical and numeric formats.

#Observation on rows: 
#(1)Some rows have unrealistic 'price'.
#(2)Some rows contain more columns with null values than others.
#(3)Some rows have null value in 'last_review' and a value of '0' in the 'availability_365' column, indicating that the property is not available.
#To ensure the accuracy and reliability of our analysis, these rows will be removed in the upcoming cleaning process.
```

```{python}
#| echo: false

#Explanation: Column Cleaning 1 - Get the top 10 columns with the most null values

top_10_columns = listings_raw.isnull().sum(axis=0).sort_values(ascending=False)[:10].index

print("Number of null values in top 10 columns:")
print(listings_raw[top_10_columns].isnull().sum())
```

```{python}
#| echo: false

#Explanation: Column Cleaning 1 - Drop the top 10 columns

listings_raw = listings_raw.drop(columns=top_10_columns)

print("\nNumber of null values in remaining columns after dropping the top 10 columns:")
print(listings_raw.isnull().sum().sort_values(ascending=False)[:10])
```

```{python}
#| echo: false

#Explanation: Column Cleaning 2 - Convert 'room_type' and 'property_type' to categories.

cats = ['property_type','room_type']

for c in cats:
    print(f"Converting {c}")
    listings_raw[c] = listings_raw[c].astype('category')

print(f"Now {cats[0]} is of type '{listings_raw[cats[0]].dtype}'", "\n") 
print(listings_raw[cats[0]].cat.categories.values)

print(f"Now {cats[1]} is of type '{listings_raw[cats[1]].dtype}'", "\n") 
print(listings_raw[cats[1]].cat.categories.values)
```

```{python}
#| echo: false

#Explanation: Column Cleaning 3 - Convert 'price' to numeric format.

money = ['price']

for m in money:
    print(f"Converting {m}")
    listings_raw[m] = listings_raw[m].str.replace('$','', regex=False).str.replace(',','').astype('float')

listings_raw.sample(10, random_state=42) [money]
```

```{python}
#| echo: false

# Explanation: Column Cleaning 4 - Convert other object columns to integer format.

ints = ['id', 'host_id', 'host_listings_count', 'host_total_listings_count', 'accommodates', 
        'beds', 'minimum_nights', 'maximum_nights', 'availability_365']

for i in ints:
    print(f"Converting {i}")
    try:
        listings_raw[i] = listings_raw[i].astype('float').astype('int')
    except ValueError as e:
        print(" - !!!Converting to unsigned 16-bit integer!!!")
        listings_raw[i] = listings_raw[i].astype('float').astype(pd.UInt16Dtype())
```

```{python}
#| echo: false

# Explanation: Row Cleaning 1 - Exclude listings with unrealistic prices.

count_price_expensive = (listings_raw['price'] > 2000).sum()
print(f"Number of rows with price greater than 2000: {count_price_expensive}")

count_price_zero = (listings_raw['price'] == 0).sum()
print(f"Number of rows with price equal to 0: {count_price_zero}")

count_price_missing = listings_raw['price'].isna().sum()
print(f"Number of rows with missing price: {count_price_missing}")

listings_raw = listings_raw[
    ~(  # Negate the combined conditions
        (listings_raw['price'] > 2000) |  
        (listings_raw['price'] == 0) |   
        (listings_raw['price'].isna())   
    )
]

count_price_expensive = (listings_raw['price'] > 2000).sum()
print(f"Number of rows with price greater than 2000 after cleaning: {count_price_expensive}")

count_price_zero = (listings_raw['price'] == 0).sum()
print(f"Number of rows with price equal to 0 after cleaning: {count_price_zero}")

count_price_missing = listings_raw['price'].isna().sum()
print(f"Number of rows with missing price after cleaning: {count_price_missing}")
```

```{python}
#| echo: false

# Explanation: Row Cleaning 2 - Calculate the number of missing values per row and plot a histogram of the distribution

probs = listings_raw.isnull().sum(axis=1) 
print(type(probs)) 
probs.plot.hist(bins=30)
```

```{python}
#| echo: false

#Explanation: Row Cleaning 2 - Drop rows where there are 10 or more missing values

cutoff = 10
listings_raw.drop(probs[probs > cutoff].index, inplace=True)
print(f"Have reduced data frame to: {listings_raw.shape[0]:,} rows and {listings_raw.shape[1]:,} columns")
```

```{python}
#| echo: false

#Explanation: Row Cleaning 3 - Drop rows where 'last_review' = Null

dropped_rows = len(listings_raw) - len(listings_raw.dropna(subset=['last_review']))
listings_raw = listings_raw.dropna(subset=['last_review'])
print(f"Number of rows dropped: {dropped_rows}")
```

```{python}
#| echo: false

#Explanation: Data cleaning validation (Seeded Rows)

listings_raw.sample(10, random_state=42)
```

```{python}
#| echo: false

#Explanation: Data cleaning validation (Column Type)

listings_clean = listings_raw
listings_clean.info()
```

```{python}
#| echo: false

#Explanation: Save the clean data locally

path = os.path.join('Data','Clean')

if not os.path.exists(path):
    print(f"Creating {path} under {os.getcwd()}") 
    os.makedirs(path)
    
listings_clean.to_csv(os.path.join(path,fn), index=False) 
print("Done.")
```

As of December 2023, data from Inside Airbnb shows approximately 66,000 active listings in London. These listings are categorized into types such as Entire home/apartment, Private room, Shared room, and Hotel room, with Entire home/apartment listings having the most significant impact on the local rental market. Such properties are often removed from the long-term rental (LTR) market and converted into short-term rentals (STRs), reducing housing supply and contributing to rising rents. In London, about 43,000 listings are categorized as Entire home/apartment, accounting for approximately 64.01% of all Airbnb listings. This constitutes a substantial share compared to other room types, as illustrated in the bar chart below (Fig 1), which compares the counts of each listing type. Furthermore, 40.09% of Entire home/apartment hosts manage more than one listing, highlighting the commercial and speculative nature of Airbnb lettings in London and their potential to disrupt the local LTR market.

```{python}
#| echo: false

#Explanation: Calculate the total number of listings in the cleaned data frame.

num_rows = listings_clean.shape[0]
print(f"Number of rows: {num_rows}")
```

```{python}
#| echo: false

#Explanation: Calculate the number of listings for each room type and the proportion of 'Entire home/apt'.

room_type_counts = listings_clean['room_type'].value_counts()

print(room_type_counts)

# Count the total number of listings for each room type
room_type_counts = listings_raw['room_type'].value_counts()

# Calculate the proportion of 'Entire home/apt' relative to the total
entire_home_proportion = room_type_counts['Entire home/apt'] / room_type_counts.sum()

print(f"Proportion of 'Entire home/apt' listings: {entire_home_proportion:.4f}")
```

```{python}
#| echo: false

#Explanation: Assuming uniform distribution, this code performs a chi-squared test to confirm the count of 'Entire home/apt' is significantly higher than expected.

# Observed values
observed = room_type_counts.values

# Null hypothesis: The distribution of property types aligns with an equal distribution.
expected = [sum(observed) / len(observed)] * len(observed)

# Chi-square test
chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)

# Print
print(f"Chi-squared Statistic: {chi2_stat}")
print(f"P-value: {p_value}")

# Result
if p_value < 0.05:
    print("Reject the null hypothesis: 'Entire home/apt' is significantly higher than expected.")
else:
    print("Fail to reject the null hypothesis: The distribution matches the expected values.")
```

```{python}
#| echo: false
#| output: asis

#Explanation: Plot the bar chart of the distribution of listings by room type.

from bokeh.io import output_notebook, show
from bokeh.plotting import figure
from bokeh.models import ColumnDataSource, HoverTool
from bokeh.palettes import Spectral4

room_types = room_type_counts.index.to_list()
counts = room_type_counts.values.tolist()

source = ColumnDataSource(data=dict(
    rt=room_types,
    count=counts,
))

# Hover tool
TOOLTIPS = [
    ("Room Type", "@rt"),
    ("Number of Listings", "@count{,}"),
]

# Bokeh bar chart customisation
p = figure(x_range=room_types, height=400, tooltips=TOOLTIPS, 
           title="Fig 1. Count of Airbnb Listings by Room Type in London (Up to December 10, 2023)",
           toolbar_location=None)
p.vbar(x='rt', top='count', width=0.9, source=source, color='purple',alpha=0.5)
p.line([-0.5, len(room_types)-0.5], [expected[0], expected[0]], 
       line_width=2, color="black", line_dash="dashed", legend_label="Mean")
p.xgrid.grid_line_color = None
p.y_range.start = 0
p.legend.location = "top_right"
p.legend.orientation = "horizontal"
p.xaxis.axis_label = "Room Type"
p.yaxis.axis_label = "Count"

# Show the plot
output_notebook()  
show(p)  
```

```{python}
#| echo: false

#Explanation: Calculate the proportion of 'Entire home/apt' hosts as multiple-listing hosts

entire_home_apt = listings_clean[listings_clean['room_type'] == 'Entire home/apt']
more_than_1_entire = entire_home_apt[entire_home_apt['calculated_host_listings_count'] != 1]

entire_total_hosts = len(entire_home_apt['host_name'].unique())
entire_more_than_1_hosts = len(more_than_1_entire['host_name'].unique())
entire_ratio = entire_more_than_1_hosts / entire_total_hosts

print(f"Entire home/apt multiple listing host ratio: {entire_ratio:.2%}")
```

Although London's [90-day rule](https://www.london.gov.uk/programmes-strategies/housing-and-land/improving-private-rented-sector/guidance-short-term-and-holiday-lets-london) restricts STRs of entire homes to 90 nights per calendar year, enforcement remains weak. The histogram below (Fig. 2) shows the distribution of available days for Entire home/apartment listings in London in the next calendar year (starting from December 10, 2023). 65.23% of these listings exceed the 90-day limit. The median number of available days in the next calendar year is 167 days, with the maximum availability being for the entire year. These figures showcase widespread non-compliance with the 90-day rule among Entire home/apartment hosts. One contributing factor is that fewer than [5%](https://airbtics.com/airbnb-rules-in-greater-london-united-kingdom/) of Airbnb listings in London display a registration number, meaning most hosts operate without regulation. This highlights the regulatory gaps in London's STR market.

```{python}
#| echo: false
#| output: asis

#Explanation: Create the histogram showing the distribution of 'availability_365' for 'Entire home/apt' in London

from bokeh.models import ColumnDataSource, Label

availability_365_data = entire_home_apt[entire_home_apt['availability_365'] > 0]['availability_365']

hist, edges = np.histogram(availability_365_data, bins=30)

source = ColumnDataSource(data=dict(
    top=hist,
    left=edges[:-1],  
    right=edges[1:],  
))

# Calculate the percentage of data beyond 90
count_beyond_90 = (availability_365_data > 90).sum()
valid_rows = len(availability_365_data)
percentage_beyond_90 = (count_beyond_90 / valid_rows) * 100

# Bokeh bar chart customisation
p = figure(title="Fig 2. Distribution of Annual Available Days for Entire Home/Apt in London",
           x_axis_label="Available Days in the Next Calendar Year from December 10, 2023",
           y_axis_label="Frequency",
           tools="pan,box_zoom,reset,hover")
p.quad(top='top', bottom=0, left='left', right='right', source=source, color='purple', alpha=0.7)
p.line([90, 90], [0, max(hist)], line_width=2, color="black", line_dash="dashed", legend_label="90 Days")
label = Label(x=95, y=max(hist) * 0.8, text=f'{percentage_beyond_90:.2f}% above 90 days', 
              text_font_size="12px", text_color="black")
p.add_layout(label)
p.xgrid.grid_line_color = None
p.y_range.start = 0
p.tools = []  # Clear hover

output_notebook()  
show(p)
```

This section highlights the speculative and unregulated nature of Entire home/apartment Airbnb listings in London, where weak enforcement of existing regulations may incentivize speculative behavior. This raises the question of what might happen if the "90-day rule" were strictly enforced. The next section will explore this scenario in greater detail.

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?

::: {.duedate}

( 45 points; Answer due {{< var assess.group-date >}} )

```{python}
#| echo: false

#Explanation: Loading London Borough boundaries (polygon)

import geopandas as gpd
ddir = os.path.join('data','geo') 
spath = 'https://github.com/jreades/fsds/blob/master/data/src/' 
boros = gpd.read_file(cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )
print('Done!')
boros.to_file('test.gpkg', driver='GPKG')
print(str(boros.geometry.iloc[1])[:50] + "...")
print(boros.crs)
```

```{python}
#| echo: false

#Explanation: Loading Airbnb listings (point)

gdf = gpd.GeoDataFrame(listings_clean,
                       geometry=gpd.points_from_xy(listings_clean.longitude, listings_clean.latitude, crs='epsg:4326'))
print(type(gdf))
print(gdf.geometry.iloc[1])
gdf = gdf.to_crs('epsg:27700') 
print(gdf.geometry.crs) #Reprojection
```

```{python}
#| echo: false

#Explanation: Subsetting the listings (points) to 'Entire home/apt' with 'availability_365' larger than 90.

entire_home_apt_90 = gdf[(gdf['room_type'] == 'Entire home/apt') & 
                          (gdf['availability_365'] > 90) & 
                          (gdf['availability_365'].notna())]
```

```{python}
#| echo: false
#| output: asis

#Explanation: Mapping the spatial distribution of Entire home/apt with more than 90 days available in the next calendar year.

import geopandas as gpd
from bokeh.plotting import figure, show
from bokeh.models import GeoJSONDataSource, HoverTool
import json

# Reproject the GeoDataFrames to EPSG:4326 (WGS84, lat/lon)
boros = boros.to_crs('EPSG:4326')  # Reproject boroughs to WGS84 (lat/lon)
entire_home_apt_90 = entire_home_apt_90.to_crs('EPSG:4326')  # Reproject points to WGS84 (lat/lon)

# Convert GeoDataFrames into GeoJSONDataSource
def get_geodatasource(gdf):    
    """Convert a GeoDataFrame into GeoJSONDataSource for Bokeh"""
    json_data = json.dumps(json.loads(gdf.to_json()))
    return GeoJSONDataSource(geojson=json_data)

# Prepare the data for GeoJSON plotting
boros_geojson = get_geodatasource(boros)  # GeoJSON for borough polygons
entire_home_apt_90_geojson = get_geodatasource(entire_home_apt_90)  # GeoJSON for points

# Create a Bokeh figure
p = figure(title='Map 1. Entire Home/Apt Listings With More Than 90 Available Days in London (2023-2024)',
           height=700, width=850, toolbar_location='right', tools='wheel_zoom,pan,reset,hover')
p.patches('xs', 'ys', source=boros_geojson, fill_alpha=0.5, line_width=0.5, line_color='black', fill_color='grey')
p.scatter(x='longitude', y='latitude', source=entire_home_apt_90_geojson, size=2, color='purple', legend_label='Entire home/apt > 90 days')

# Add hover tool for the points showing the number of available days
hover = p.select(dict(type=HoverTool))
hover.tooltips = [("Available Days", "@availability_365")]

# Add labels, title, and legend
p.title.text_font_size = '16pt'
p.legend.title = 'Listings'
p.xaxis.visible = False
p.yaxis.visible = False
p.xgrid.grid_line_color = None
p.ygrid.grid_line_color = None

output_notebook()
show(p, notebook_handle=True)
```

```{python}
#| echo: false

print(boros.columns)
```

```{python}
#| echo: false

#Explanation: Perform a spatial join to assign borough information to listings and count entire home/apt listings per borough

# Spatially join listings to boroughs to determine which borough each listing falls within
joined = gpd.sjoin(entire_home_apt_90, boros, how='left', predicate='within')

# Group by 'GSS_CODE' and count the number of points in each group
gss_code_counts = joined.groupby('GSS_CODE').size().reset_index(name='no_entire_home_apt_90')

# Merge the count back to the 'boros' GeoDataFrame
boros = boros.merge(gss_code_counts, on='GSS_CODE', how='left')

# Display the first 5 rows
print(boros.head())  
```

```{python}
#| echo: false

#Dependent Variable: Mean monthly rent per bedroom (in entire properties, excluding rooms in shared houses) by borough.

# Define the cache_data function to always re-download the file
def cache_data(url, directory):
    os.makedirs(directory, exist_ok=True)
    file_path = os.path.join(directory, os.path.basename(url))
    response = requests.get(url)
    response.raise_for_status()
    with open(file_path, 'wb') as file:
        file.write(response.content)
    return file_path

# Remote downloading 
url1 = "https://www.ons.gov.uk/file?uri=/economy/inflationandpriceindices/adhocs/2417privaterentalmarketinlondonoctober2023toseptember2024/londonrentalstatsaccessibleq32024.xlsx"
file_path1 = cache_data(url1, os.path.join('Data', 'Regression'))

sheet_names = pd.ExcelFile(file_path1).sheet_names
print("Sheet names:", sheet_names)
sheet_data = pd.read_excel(file_path1, sheet_name='2', skiprows=2) if '2' in sheet_names else pd.read_excel(file_path3, sheet_name=4, skiprows=2)
print(sheet_data.head())

# Load the sheet '2' with skiprows=2
sheet_data = pd.read_excel(file_path1, sheet_name='2', skiprows=2)
sheet_data['Mean'] = pd.to_numeric(sheet_data['Mean'], errors='coerce')

# Loop through each borough to calculate mean_rent
results = []

for borough in sheet_data['Borough'].unique():
    borough_data = sheet_data[sheet_data['Borough'] == borough]
    
# Extract mean values for each category
    studio_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Studio', 'Mean'].dropna().values
    one_bedroom_mean = borough_data.loc[borough_data['Bedroom Category'] == 'One Bedroom', 'Mean'].dropna().values
    two_bedrooms_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Two Bedrooms', 'Mean'].dropna().values / 2
    three_bedrooms_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Three Bedrooms', 'Mean'].dropna().values / 3
    four_or_more_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Four or More Bedrooms', 'Mean'].dropna().values / 4

# Calculate the mean_rent
    mean_rent = (
        (studio_mean[0] if len(studio_mean) > 0 else 0) +
        (one_bedroom_mean[0] if len(one_bedroom_mean) > 0 else 0) +
        (two_bedrooms_mean[0] if len(two_bedrooms_mean) > 0 else 0) +
        (three_bedrooms_mean[0] if len(three_bedrooms_mean) > 0 else 0) +
        (four_or_more_mean[0] if len(four_or_more_mean) > 0 else 0)
    ) / 5

# Append the result
    results.append({
        'Name': borough,
        'mean_rent': mean_rent
    })

boros_rent = pd.DataFrame(results)
print(boros_rent)

# Left join to boros
boros = boros.merge(
    boros_rent,
    left_on='NAME',
    right_on='Name',
    how='left'
)

# Drop the redundant column
boros.drop(columns=['Name'], inplace=True)
print(boros.head())
```

As of 2024, the average monthly rent for a single room in London is £1,013, which accounts for approximately [**52%**](https://www.thetimes.com/article/graduates-spend-more-than-half-their-wages-on-rent-2d5sxwvv2) of a graduate’s monthly income. The map below illustrates the average monthly rent per bedroom in entire properties (excluding shared accommodations) across London boroughs. **Kensington and Chelsea** has the highest average rent at £1,982 per month, followed by **Westminster** (£1,868) and the **City of London** (£1,460).

Airbnb listings for entire homes are often criticized for driving up rents in the LTR market by converting properties into STRs, reducing LTR housing availability. As noted earlier, most Airbnb hosts in London with entire-home listings manage multiple properties and rent them for over 90 days per year, reflecting a shift from optimizing underutilized properties to profit-driven operations that disrupt the LTR market [@duso_airbnb_2024]. To address this, London’s [90-day rule](https://www.london.gov.uk/programmes-strategies/housing-and-land/improving-private-rented-sector/guidance-short-term-and-holiday-lets-london) limits STRs of entire homes to 90 days annually without planning permission. However, less than [5%](https://airbtics.com/airbnb-rules-in-greater-london-united-kingdom/#:~:text=Less%20than%205%25%20of%20Airbnb,space%20for%20short%2Dterm%20rentals.) of Airbnb listings display a registration number, making enforcement difficult for local authorities.

We analyzed how Airbnb entire-home listings operating over 90 days a year affect LTR rents, using *Inside Airbnb*’s December 2023 dataset for London. The analysis accounts for borough-level factors such as socio-demographics, LTR demand and supply, and local amenities. Below are these factors with links to their data sources:

**Socio-demographic**: [Population Density](https://data.london.gov.uk/dataset/land-area-and-population-density-ward-and-borough), [Median Income](https://data.london.gov.uk/dataset/average-income-tax-payers-borough), [Unemployment Rate](https://www.ethnicity-facts-figures.service.gov.uk/work-pay-and-benefits/unemployment-and-economic-inactivity/unemployment/latest/#download-the-data), [Crime Rate](https://data.london.gov.uk/dataset/recorded_crime_summary)  
**LTR Demand & Supply**: [Percentage of Renters](https://data.london.gov.uk/dataset/housing-tenure-borough), [Housing Stock per Person](https://data.london.gov.uk/dataset/local-authority-housing-stock)  
**Local Amenities**: [Education](https://data.london.gov.uk/dataset/gcse-results-by-borough), [Public Transport Accessibility](https://data.london.gov.uk/dataset/public-transport-accessibility-levels), [Green Space](https://data.london.gov.uk/dataset/green-and-blue-cover)  

```{python}
#| echo: false

#Explanation: Plot the distribution of mean_rent and log-transform it if it is skewed

# Plot the histogram
plt.figure(figsize=(10, 6))
plt.hist(boros['mean_rent'].dropna(), bins=20, color='skyblue', edgecolor='black')

# Add labels and title
plt.title('Distribution of Mean Rent', fontsize=16)
plt.xlabel('Mean Rent', fontsize=14)
plt.ylabel('Frequency', fontsize=14)

# Show the plot
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Log-transformation
boros['log_mean_rent'] = np.log(boros['mean_rent'] + 1)  # Add 1 to avoid log(0)
print(boros.head())
```

```{python}
#| echo: false

#Explanation: Number of households per borough to calculate the independent variable

url2 = "https://data.london.gov.uk/download/housing-tenure-borough/785f6f0e-cc4b-42fd-8093-597b009555f2/tenure-households-borough.xlsx"

def cache_data(url, cache_path):
    if not os.path.exists(cache_path):
        os.makedirs(cache_path)
    local_file = os.path.join(cache_path, os.path.basename(url))
    if not os.path.exists(local_file):
        # Download the file if not cached
        import urllib.request
        urllib.request.urlretrieve(url, local_file)
    return local_file

# Extract the sheet and load it
boros_household_raw = pd.read_excel(
    cache_data(url2, os.path.join('Data', 'Regression')),
    sheet_name='2020', 
    header=[0, 1, 2]
)

# Flatten multi-level column names
boros_household_raw.columns = ['_'.join(col).strip() for col in boros_household_raw.columns.values]
print("Flattened columns:", boros_household_raw.columns)

# Select relevant columns (first and seventh)
boros_household = boros_household_raw.iloc[:, [0, 6]]

# Rename columns for clarity
boros_household.columns = ['Unnamed_0', 'no_household']

# Merge boros_household with boros
boros = boros.merge(
    boros_household,
    left_on='GSS_CODE',
    right_on='Unnamed_0',
    how='left'
)

# Drop the redundant column
boros.drop(columns=['Unnamed_0'], inplace=True)
print(boros.head())
```

```{python}
#| echo: false

#Key Independent Variable: Number of Entire home/apt with annual availability > 90 per 1000 households by borough & Log-transformation

url2 = "https://data.london.gov.uk/download/housing-tenure-borough/785f6f0e-cc4b-42fd-8093-597b009555f2/tenure-households-borough.xlsx"

def cache_data(url, cache_path):
    if not os.path.exists(cache_path):
        os.makedirs(cache_path)
    local_file = os.path.join(cache_path, os.path.basename(url))
    if not os.path.exists(local_file):
        # Download the file if not cached
        import urllib.request
        urllib.request.urlretrieve(url, local_file)
    return local_file

# Extract the sheet and load it
boros_household_raw = pd.read_excel(
    cache_data(url2, os.path.join('Data', 'Regression')),
    sheet_name='2020', 
    header=[0, 1, 2]
)

# Flatten multi-level column names
boros_household_raw.columns = ['_'.join(col).strip() for col in boros_household_raw.columns.values]
print("Flattened columns:", boros_household_raw.columns)

# Select relevant columns (first and seventh)
boros_household = boros_household_raw.iloc[:, [0, 6]]

# Rename columns for clarity
boros_household.columns = ['Unnamed_0', 'no_household']

# Merge boros_household with boros
boros = boros.merge(
    boros_household,
    left_on='GSS_CODE',
    right_on='Unnamed_0',
    how='left',
    suffixes=('', '_household')
)

# Drop the redundant column
boros.drop(columns=['Unnamed_0'], inplace=True)
print(boros.head())

# Check and convert columns to numeric
boros['no_entire_home_apt_90'] = pd.to_numeric(boros['no_entire_home_apt_90'], errors='coerce')
boros['no_household'] = pd.to_numeric(boros['no_household'], errors='coerce')

# Create the new column
boros['entire_1000_household'] = (boros['no_entire_home_apt_90'] / boros['no_household']) * 1000
boros['log_entire_1000_household'] = np.log(boros['entire_1000_household'] + 1)

# Display the first few rows to verify
print(boros.head())
```

```{python}
#| echo: false

#Explanation: Control 1 - Population Density

url3 = "https://data.london.gov.uk/download/land-area-and-population-density-ward-and-borough/77e9257d-ad9d-47aa-aeed-59a00741f301/housing-density-borough.csv"

boros_population_density = pd.read_csv(cache_data(url3, os.path.join('Data', 'Regression')))

boros_population_density.info(verbose=True)

#Filter boros_population_density for the year 2023 and select relevant columns
boros_population_density_filtered = (
    boros_population_density
    .loc[boros_population_density['Year'] == 2023, 
         ['Code', 'Population', 'Population_per_square_kilometre']]
    .reset_index(drop=True)
)

if 'Code' in boros.columns:
    boros = boros.drop(columns=['Code'])

#Left join
boros = boros.merge(
    boros_population_density_filtered, 
    left_on='GSS_CODE', 
    right_on='Code', 
    how='left', 
    suffixes=('', '_density')
)

#Drop the 'Code' column from the merged DataFrame if it exists
if 'Code' in boros.columns:
    boros = boros.drop(columns=['Code'])

#Display the first few rows of the final DataFrame
print(boros.head())
```

```{python}
#| echo: false

#Explanation: Control 2 - Median Taxpayer Income

url4 = "https://data.london.gov.uk/download/average-income-tax-payers-borough/392e86d4-f1d3-4f06-a6a5-7fcd0fd65948/income-of-tax-payers.xlsx"

boros_income_raw = pd.read_excel(cache_data(url4, os.path.join('Data', 'Regression')), 
                                 sheet_name='Total Income', header=[0, 1])


boros_income_code = boros_income_raw[('Unnamed: 0_level_0', 'Code')]

boros_income_median = boros_income_raw[('2021-22', 'Median £')]

boros_income = pd.DataFrame({
    'Code': boros_income_code,
    'Median £': boros_income_median
})

boros_income = boros_income.reset_index(drop=True)

#Perform the left join with the 'boros' DataFrame
boros = boros.merge(boros_income, left_on='GSS_CODE', right_on='Code', how='left')

print(boros.head())
```

```{python}
#| echo: false

#Explanation: Control 3 - Unemployment Rate

url5 = "https://www.ethnicity-facts-figures.service.gov.uk/work-pay-and-benefits/unemployment-and-economic-inactivity/unemployment/latest/downloads/unemployment-by-local-authorities.csv"
local_path5 = os.path.join('Data', 'Regression')

unemployment_data_raw = pd.read_csv(cache_data(url5, local_path5))

unemployment_data_filtered = unemployment_data_raw[
    (unemployment_data_raw['time'] == 'Jan2022-Dec2022') &
    (unemployment_data_raw['ethnicity'] == 'All') &
    (unemployment_data_raw['ethnicity_type'] == 'All')
]
unemployment_data_filtered = unemployment_data_filtered[['Geography', 'value']]

# Perform left join
boros = boros.merge(
    unemployment_data_filtered,
    how='left',
    left_on='NAME',
    right_on='Geography'
)
boros.rename(columns={'value': 'unemployment_rate'}, inplace=True)

boros.drop(columns=['Geography'], inplace=True)
print(boros)
```

```{python}
#| echo: false

#Explanation: Control 4 - Crime Rate

url6 = "https://data.london.gov.uk/download/recorded_crime_summary/d234c4fe-b060-46bd-a2a6-c5dd1119bddd/MPS%20Borough%20Level%20Crime%20%28most%20recent%2024%20months%29.csv"
local_path6 = os.path.join('Data', 'Regression')

crime_rate_raw = pd.read_csv(cache_data(url6, local_path6))

# Select only columns with '2023'
crime_2023 = crime_rate_raw.filter(like='2023', axis=1)  # Select columns containing '2023'
crime_2023['BoroughName'] = crime_rate_raw['BoroughName']  # Keep the 'BoroughName' column

# Group by 'BoroughName' and calculate the sum
crime_grouped = crime_2023.groupby('BoroughName').sum(numeric_only=True)  # Sum numeric columns
crime_grouped = crime_grouped.reset_index()  # Reset the index to make 'BoroughName' a column
crime_grouped['crime_count'] = crime_grouped.sum(axis=1, numeric_only=True)  # Sum all 2023 columns for each borough

boros_crime = crime_grouped[['BoroughName', 'crime_count']]
print(boros_crime.head())

boros = boros.merge(
    boros_crime,
    how='left',
    left_on='NAME',
    right_on='BoroughName',
    suffixes=('', '_crime')  
)

boros['crime_rate'] = boros['crime_count'] / boros['Population']
print(boros.head())
```

```{python}
#| echo: false

#Explanation: Control 5 - Housing Demand (% of Renters)

url7 = "https://data.london.gov.uk/download/housing-tenure-borough/53cb03c2-3e99-4d71-937b-90b95a64ebaa/tenure-population-borough.csv"
local_path7 = os.path.join('Data', 'Regression')

tenure_data_raw = pd.read_csv(cache_data(url7, local_path7))

tenure_data_raw['Percent_of_population_in_borough'] = pd.to_numeric(
    tenure_data_raw['Percent_of_population_in_borough'], errors='coerce'
)
tenure_filtered = tenure_data_raw[
    (tenure_data_raw['Year'] == 2018) & 
    (tenure_data_raw['Tenure'].isin([
        'Rented.from.Local.Authority.or.Housing.Association',
        'Rented.from.Private.landlord'
    ]))
]

tenure_pivot = tenure_filtered.pivot_table(
    index='Code', 
    columns='Tenure', 
    values='Percent_of_population_in_borough',
    aggfunc='sum'  
).reset_index()

# Renters in both social housing and private housing are considered
tenure_pivot.columns.name = None  
tenure_pivot.rename(columns={
    'Rented.from.Local.Authority.or.Housing.Association': 'percent_local_authority_housing',
    'Rented.from.Private.landlord': 'percent_private_landlord'
}, inplace=True)

# Calculate the sum for the two categories
tenure_pivot['percent_of_renters'] = (
    tenure_pivot['percent_local_authority_housing'] + tenure_pivot['percent_private_landlord']
)

boros_renters = tenure_pivot[['Code', 'percent_local_authority_housing', 'percent_private_landlord', 'percent_of_renters']]
print(boros_renters.head())

boros = boros.merge(
    boros_renters[['Code', 'percent_of_renters']],  
    how='left',  
    left_on='GSS_CODE',  
    right_on='Code'  
)

print(boros.head())
```

```{python}
#| echo: false

#Explanation: Control 6 - Housing Supply (Housing stock per person)

url8 = "https://data.london.gov.uk/download/local-authority-housing-stock/3421002e-f0ec-45e5-bf73-f594788ec93d/local-authority-housing-stock-borough.csv"
local_path8 = os.path.join('Data', 'housing-stock-by-local-authorities.csv')

housing_stock_data_raw = pd.read_csv(cache_data(url8, local_path8))
boros_housing_stock = housing_stock_data_raw[['Number of properties 2020', 'Code']].copy()
boros_housing_stock['Number of properties 2020'] = boros_housing_stock['Number of properties 2020'].str.replace(',', '').astype(float)
boros_housing_stock.rename(columns={'Number of properties 2020': 'housing_stock_2020'}, inplace=True)

boros_housing_stock.reset_index(drop=True, inplace=True)
print(boros_housing_stock.head())

boros = boros.merge(
    boros_housing_stock[['Code', 'housing_stock_2020']],  # Select only necessary columns
    how='left',
    left_on='GSS_CODE',
    right_on='Code',
    suffixes=('', '_housing_stock')  # Specify suffixes to avoid column name conflicts
)

if 'Code' in boros.columns:
    boros = boros.drop(columns=['Code'])

boros['number_of_dwellings_per_person'] = boros['housing_stock_2020'] / boros['Population']
print(boros.head())
```

```{python}
#| echo: false

#Explanation: Control 7 - Education (Average 8 score)

url9 = "https://data.london.gov.uk/download/gcse-results-by-borough/a8a71d73-cc48-4b30-9eb5-c5f605bc845c/GCSE%20results%20by%20gender.xlsx"
local_path9 = os.path.join('Data', 'Regression')
file_path9 = cache_data(url9, local_path9)

education_data = pd.read_excel(file_path9, sheet_name='2022-23', skiprows=4)
education_data.rename(columns={
    'E09000001': 'Code',
    'Unnamed: 3': 'Average Attainment 8 score per pupil'
}, inplace=True)

boros_education = education_data[['Code', 'Average Attainment 8 score per pupil']].copy()
boros_education.dropna(subset=['Code'], inplace=True)
boros_education.reset_index(drop=True, inplace=True)

print(boros_education.head())

boros = boros.merge(
    boros_education,  
    how='left',  
    left_on='GSS_CODE',  
    right_on='Code'  
)

if 'Code' in boros.columns:
    boros.drop(columns=['Code'], inplace=True)

print(boros.head())
```

```{python}
#| echo: false

#Explanation: Control 8 - Public Transport (PTAL) by borough

url10 = "https://data.london.gov.uk/download/public-transport-accessibility-levels/8e520b81-dd06-4ce6-aaa0-972dccf84b57/Borough%20AvPTAI2015.csv"
local_path10 = os.path.join('Data', 'Regression')
file_path10 = cache_data(url10, local_path10)

boros_PTA = pd.read_csv(file_path10)[['Borough Code', 'AvPTAI2015']].copy()
boros = boros.merge(
    boros_PTA,  
    how='left',  
    left_on='GSS_CODE',  
    right_on='Borough Code'  
)

if 'Borough Code' in boros.columns:
    boros.drop(columns=['Borough Code'], inplace=True)

print(boros.head())
```

```{python}
#| echo: false

#Explanation: Control 9 - Green Space (% of area as green and blue space) by borough

url11 = "https://data.london.gov.uk/download/green-and-blue-cover/fdff7445-bb87-4584-ada8-d527e7fcec97/green_cover_borough_summary_0.05.xlsx"
local_path11 = os.path.join('Data', 'Regression')
file_path11 = cache_data(url11, local_path11)

boros_green_blue = pd.read_excel(file_path11, sheet_name='borough_green_cover')[['lb_name', 'percent_green+blue']].copy()
boros = boros.merge(
    boros_green_blue,  
    how='left',  
    left_on='GSS_CODE',  
    right_on='lb_name'  
)

if 'lb_name' in boros.columns:
    boros.drop(columns=['lb_name'], inplace=True)
print(boros.head())
```

```{python}
#| echo: false

#Explanation: Select only the key columns for regression

print(boros.columns)
key_vars = [
    'NAME', 'GSS_CODE', 'log_mean_rent', 'log_entire_1000_household',
    'Population_per_square_kilometre', 'Median £', 'crime_rate',
    'percent_of_renters', 'number_of_dwellings_per_person', 'AvPTAI2015',
    'percent_green+blue', 'ONS_INNER', 'Population', 'HECTARES', 'geometry'
]

boros = boros[key_vars]
print(boros.head())
print(boros.dtypes)
```

```{python}
#| echo: false

#Explanation: Convert Inner/Outer as a numeric binary

boros['ONS_INNER'] = boros['ONS_INNER'].map({'T': 1, 'F': 0})
print(boros.head())
```

```{python}
#| echo: false

#Explanation: Drop 'City of London' and check data frame

boros = boros.drop(index=32)
print(boros)
```

```{python}
#| echo: false

#Explanation: OLS Regression

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import pandas as pd
import statsmodels.api as sm

# Define VIF Function
def drop_column_using_vif_(df, thresh=5, protect=[]): 
    while True:
        df_with_const = add_constant(df)
        vif_df = pd.Series(
            [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])],
            name="VIF",
            index=df_with_const.columns
        ).to_frame()
        vif_df = vif_df.drop('const')  # Drop the constant from consideration

        # Find the variable with the highest VIF
        vif_to_drop = vif_df.loc[~vif_df.index.isin(protect)]  # Exclude protected variables
        if vif_to_drop.VIF.max() > thresh:
            index_to_drop = vif_to_drop.index[vif_to_drop.VIF == vif_to_drop.VIF.max()].tolist()[0]
            print(f"Dropping: {index_to_drop}")
            df = df.drop(columns=index_to_drop)
        else: 
            break
    return df

# Select predictors and remove non-numeric columns
predictors = boros.drop(columns=['NAME', 'GSS_CODE', 'HECTARES', 'geometry', 'Population', 'log_mean_rent'])
predictors = predictors.apply(pd.to_numeric, errors='coerce')  

# Run VIF test
df_predictors_selected_VIF = drop_column_using_vif_(predictors, thresh=5, protect=['log_entire_1000_household'])

print("The columns remaining after VIF selection are:")
print(df_predictors_selected_VIF.columns)

# Dependent variable
endog = boros.loc[df_predictors_selected_VIF.index, 'log_mean_rent']

# Independent variables with constant added
exog = sm.add_constant(df_predictors_selected_VIF)

# Perform the OLS regression
OLS_regression_data = sm.OLS(endog=endog, exog=exog).fit()

# Print the regression summary
print(OLS_regression_data.summary())
```

By controlling for borough-level factors, we find that a **1%** increase in Airbnb entire-home listings operating over 90 days per 1,000 households results in a **0.28%** rise in mean rent. This shows the potential for reducing rents through stricter regulation of such listings in London.

```{python}
#| echo: false

#Explanation: Unlog the column (assuming log transformation was natural log: log(x + 1))

boros['entire_1000_household'] = np.exp(boros['log_entire_1000_household']) - 1
print(boros)
```

```{python}
#| echo: false

#Explanation: Conduct Elbow Test & Clustering

# Prepare the data for clustering
from sklearn.cluster import KMeans
X = boros[['entire_1000_household']].values  # Use the unlogged column for clustering

# Perform the elbow test
wcss = []
for k in range(1, 11):  # Test cluster sizes from 1 to 10
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot the elbow curve
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.show()
```

```{python}
#| echo: false

#Explanation: Plot

optimal_clusters = 3
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
boros['cluster'] = kmeans.fit_predict(X)
boros.plot(column='cluster', cmap='Set2', legend=False, figsize=(12, 8))
plt.title('Clusters of Entire Home/Apt Listings per 1000 Households by Borough')
plt.show()
```

After analysing the InsideAirbnb dataset we consider that it provides insights to establish regulations tailored specifically to London. This approach will help avoid too generic rules that may affect users that are not misusing the platform or fail to address those exploiting the platform and therefore affecting London’s housing crisis. 

For instance, our analysis reveals that only 5% of London hosts include their registration numbers on the platform, making it difficult to identify those accumulating multiple properties. To address this, and as a first step on defining regulations, we propose requiring Airbnb to verify that hosts are the actual legal representants of the properties they list. This step would improve transparency, allowing regulators to distinguish between small-scale hosts and larger ones, ensuring that regulations are enforced fairly and targeted effectively. 

Between these two kinds of hosts, the first group could be understood as people who genuinely use Airbnb as a means of generating income from spare rooms or temporarily unoccupied properties, so regulations should aim to minimize affecting them as discussed in Question 5. For instance, we suggest maintaining the 90-day rental limit and the “Rent a room” scheme which allows up to £7,500 tax-free income (KeyNest, 2019). However, those renting for more than 90 days would need to apply for a permit and pay business taxes like hotels, following San Francisco practices.

For hosts with more listings, stricter regulations should be included as they act as businesses. They should also obtain permits and pay hotel level taxes, which will help address unfair competition concerns of traditional short term rental businesses (Reyes, 2015). Other specific regulations can be informed by our clustering analysis, where we identified boroughs where Airbnb density significantly impacts the housing market. In these areas, we recommend banning such businesses. In those where there is lower Airbnb density, they will be able to operate but with a revenue cap equivalent to the boroughs’ average long-term rent. This would reduce financial incentives for prioritizing STL over long-term rentals, gradually returning properties to the housing market. 

This kind of measure aims to protect individuals using Airbnb responsibly, while applying stricter rules to those using the platform as a business at the expense of long-term housing. Ultimately, the goal would be to ease the rising housing prices by reducing the number of homes lost to short-term lets and restoring them to the long-term market. 

```{python}
from graphviz import Digraph
from IPython.display import SVG, display

# Create a Digraph object
diagram = Digraph(
    format="svg",
    graph_attr={"rankdir": "TB"},
    node_attr={"shape": "box", "fontsize": "12", "fontname": "Roboto Flex"},
)

# Add nodes and edges
diagram.node(
    "A", "ALL AIRBNB LISTINGS", fontsize="24", shape="none"
)
diagram.node(
    "B",
    "Host needs to be the real owner \n(landlord/legal occupant)",
    fontsize="10",
    fontcolor="grey",
    color="grey",
)
diagram.node(
    "C",
    "Hosts with 1 or 2 listings",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "D",
    "Renting ≤ 90 days in a year",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "E",
    "Renting > 90 days in a year",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "F", "Hosts with > 2 listings", fontsize="14", penwidth="3.0"
)
diagram.node(
    "G",
    "Permit and business taxes like hotels apply",
    fontsize="10",
    fontcolor="grey",
    color="grey",
)
diagram.node(
    "H",
    "In green (clustered) boroughs",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "I",
    "In grey (non-clustered) boroughs",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "J", "up to £7,500 tax-free income", fontsize="10", fontcolor="grey", color="grey"
)
diagram.node(
    "K",
    "Permit and business taxes like hotels apply",
    fontsize="10",
    fontcolor="grey",
    color="grey",
)
diagram.node("L", "Not allowed", fontsize="10", fontcolor="grey", color="grey")
diagram.node(
    "M",
    "Allowed under the revenue cap per each property, \nequivalent to the boroughs' average long-term rent",
    fontsize="10",
    fontcolor="grey",
    color="grey",
)

# Connect nodes
diagram.edge("A", "B", dir="none", color="grey")
diagram.edge("B", "C")
diagram.edge("B", "F")
diagram.edge("C", "D", weight="0.5")
diagram.edge("C", "E", weight="0.5")
diagram.edge("F", "G", dir="none", color="grey")
diagram.edge("G", "H")
diagram.edge("G", "I")
diagram.edge("D", "J", dir="none", color="grey")
diagram.edge("E", "K", dir="none", color="grey")
diagram.edge("H", "L", dir="none", color="grey")
diagram.edge("I", "M", dir="none", color="grey")

# Render and display the diagram in the notebook
svg_data = diagram.pipe(format="svg") 
display(SVG(svg_data))  
```

:::

## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
