---
bibliography: CASA_novas_bib.bib
csl: harvard-cite-them-right.csl
title: CASA_novas's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto Flex
    monofont: Liberation Mono
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| echo: false
#Explanation: Import libraries
import os
import pandas as pd
```

```{python}
#| echo: false
#Explanation: This code chunk downloads a file from a given URL and saves it locally to a specified destination, caching it if it doesn't already exist or is too small, and returns the local file path.

from urllib.parse import urlparse
import os
import requests  

def cache_data(src: str, dest: str) -> str:
    """
    Downloads and caches a remote file locally.

    Parameters
    ----------
    src : str
        The remote *source* for the file, any valid URL should work.
    dest : str
        The *destination* location to save the downloaded file.
        
    Returns
    -------
    str
        A string representing the local location of the file.
    """
    url = urlparse(src)
    fn = os.path.split(url.path)[-1]
    dfn = os.path.join(dest, fn)

    if not os.path.isfile(dfn) or os.stat(dfn).st_size < 250:
        print(f"{dfn} not found, downloading!")

        # Create directories if they don't exist
        os.makedirs(dest, exist_ok=True)

        try:
            response = requests.get(src)
            response.raise_for_status()  

            with open(dfn, "wb") as file:
                file.write(response.content)

            print("\tDone downloading...")

            f_size = os.stat(dfn).st_size
            print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

        except requests.exceptions.RequestException as e:
            print(f"Failed to download file from {src}")
            raise e

    else:
        print(f"Found {dfn} locally!")
        f_size = os.stat(dfn).st_size
        print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

    return dfn
```

```{python}
#| echo: false
#Explanation: The code constructs a URL to access a specific dataset from Inside Airbnb, downloads and caches the file, reads it into a Pandas DataFrame, and then prints the shape and detailed information

host = 'https://data.insideairbnb.com'
country = 'united-kingdom'
state = 'england'
city = 'london'
ymd = '2024-09-06'
url = f'{host}/{country}/{state}/{city}/{ymd}/data/listings.csv.gz'

listings_raw = pd.read_csv(cache_data(url, os.path.join('data', 'raw')))

print(f"Data frame is {listings_raw.shape[0]:,} x {listings_raw.shape[1]}")
listings_raw.info(verbose=True)
```

## 1. Who collected the InsideAirbnb data?

::: {.duedate}

( 2 points; Answer due Week 7 )

The Inside Airbnb data was primarily collected by [Murray Cox](https://twitter.com/murrayscox), a Brooklyn based independent researcher who started the project in 2015. While Cox leads the initiative, a network of volunteers, activists, and researchers supports the data collection by providing local context, refining scraping tools, and organizing the raw data.

:::


## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}

( 4 points; Answer due Week 7 )

Inside Airbnb collected data to reveal the impact of short-term lets (STL) on residential communities, often obscured by Airbnb’s PR-driven data. In 2015, Airbnb purged over 1,000 listings from multi-listing hosts before presenting a manipulated snapshot under its ‘transparency initiative’ [@cox_how_2020]. If this scandal had not been exposed, Airbnb’s harm to local housing markets would have been downplayed. As a grassroots project, Inside Airbnb challenges dominant corporate narratives by providing transparent data that highlights Airbnb’s role in gentrification, eviction, and the housing crisis. This empowers researchers, policymakers, and marginalized communities to collectively advocate for fair STL regulations and social justice—exemplifying 'data for co-liberation' [@dignazio_data_2020].
:::


## 3. How did they collect it?

::: {.duedate}

( 5 points; Answer due Week 8 )

:::

## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

( 11 points; Answer due Week 9 )

:::

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}

( 18 points; Answer due {{< var assess.group-date >}} )

:::

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 
# Section A. Data Loading
```{python}
#| echo: false
#Explanation: Import packages
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sms
import seaborn as sns
from scipy.stats import chisquare
```

```{python}
#| echo: false
# Explanation: This code chunk downloads a file from a given URL and saves it locally to a specified destination, caching it if it doesn't already exist or is too small, and returns the local file path.

from urllib.parse import urlparse
import requests  

def cache_data(src: str, dest: str) -> str:
    """
    Downloads and caches a remote file locally.

    Parameters
    ----------
    src : str
        The remote *source* for the file, any valid URL should work.
    dest : str
        The *destination* location to save the downloaded file.
        
    Returns
    -------
    str
        A string representing the local location of the file.
    """
    url = urlparse(src)
    fn = os.path.split(url.path)[-1]
    dfn = os.path.join(dest, fn)

    if not os.path.isfile(dfn) or os.stat(dfn).st_size < 250:
        print(f"{dfn} not found, downloading!")

        # Create directories if they don't exist
        os.makedirs(dest, exist_ok=True)

        try:
            response = requests.get(src)
            response.raise_for_status()  

            with open(dfn, "wb") as file:
                file.write(response.content)

            print("\tDone downloading...")

            f_size = os.stat(dfn).st_size
            print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

        except requests.exceptions.RequestException as e:
            print(f"Failed to download file from {src}")
            raise e

    else:
        print(f"Found {dfn} locally!")
        f_size = os.stat(dfn).st_size
        print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

    return dfn
```


```{python}
#| echo: false
#Explanation: The code constructs a URL to access a specific dataset from Inside Airbnb, downloads and caches the file, reads it into a Pandas DataFrame, and then prints the shape and detailed information

host = 'https://data.insideairbnb.com'
country = 'united-kingdom'
state = 'england'
city = 'london'
ymd = '2023-12-10'
url = f'{host}/{country}/{state}/{city}/{ymd}/data/listings.csv.gz'

listings_raw = pd.read_csv(cache_data(url, os.path.join('data', 'raw')))

print(f"Data frame is {listings_raw.shape[0]:,} x {listings_raw.shape[1]}")
listings_raw.info(verbose=True)
```

```{python}
#| echo: false
#Explanation: The code saves the dataset to a specified folder (Data/Raw) in the current working directory as a CSV file.

path = os.path.join('Data', 'Raw') 
fn = url.split('/')[-1] 
print(f"Writing to: {fn}")

if not os.path.exists(path):
    print(f"Creating {path} under {os.getcwd()}")
    os.makedirs(path)

if not os.path.exists(os.path.join(path, fn)): 
    listings_raw.to_csv(os.path.join(path, fn), index=False) 
    print("Done.")
```
# Section B. Data Cleaning
```{python}
#| echo: false
#Explanation: Randomly selects 5 rows from the DataFrame listings_raw using a fixed random seed for reproducibility.

listings_raw.sample(10, random_state=42)

#Observation on columns:
#(1)Some columns contain predominantly null values, such as 'license' and 'description'.
#(2)The 'property_type' and 'room_type' column should be in categorical format, not object.
#(3)Some columns, such as 'price', should be in numeric format, not object.
#Conclusion: To ensure the accuracy and reliability of our analysis, columns with predominantly null values will be dropped. Some 'object' columns will be converted to categorical and numeric formats.

#Observation on rows: 
#(1)Some rows contain more columns with null values than others.
#(2)Some rows have null value in 'last_review' and a value of '0' in the 'availability_365' column, indicating that the property is not available.
#Conclusion: To ensure the accuracy and reliability of our analysis, these rows will be removed in the upcoming cleaning process.
```
```{python}
#| echo: false
#Explanation: Column Cleaning 1 - Get the top 10 columns with the most null values

top_10_columns = listings_raw.isnull().sum(axis=0).sort_values(ascending=False)[:10].index

print("Number of null values in top 10 columns:")
print(listings_raw[top_10_columns].isnull().sum())
```
```{python}
#| echo: false
#Explanation: Column Cleaning 1 - Drop the top 10 columns

listings_raw = listings_raw.drop(columns=top_10_columns)

print("\nNumber of null values in remaining columns after dropping the top 10 columns:")
print(listings_raw.isnull().sum().sort_values(ascending=False)[:10])
```
```{python}
#| echo: false
#Explanation: Column Cleaning 2 - Convert 'room_type' and 'property_type' to categories.

cats = ['property_type','room_type']

for c in cats:
    print(f"Converting {c}")
    listings_raw[c] = listings_raw[c].astype('category')

print(f"Now {cats[0]} is of type '{listings_raw[cats[0]].dtype}'", "\n") 
print(listings_raw[cats[0]].cat.categories.values)

print(f"Now {cats[1]} is of type '{listings_raw[cats[1]].dtype}'", "\n") 
print(listings_raw[cats[1]].cat.categories.values)
```
```{python}
#| echo: false
#Explanation: Column Cleaning 3 - Convert 'price' to numeric format.

money = ['price']

for m in money:
    print(f"Converting {m}")
    listings_raw[m] = listings_raw[m].str.replace('$','', regex=False).str.replace(',','').astype('float')

listings_raw.sample(10, random_state=42) [money]
```
```{python}
#| echo: false
# Explanation: Column Cleaning 4 - Convert other object columns to integer format.

ints = ['id', 'host_id', 'host_listings_count', 'host_total_listings_count', 'accommodates', 
        'beds', 'minimum_nights', 'maximum_nights', 'availability_365']

for i in ints:
    print(f"Converting {i}")
    try:
        listings_raw[i] = listings_raw[i].astype('float').astype('int')
    except ValueError as e:
        print(" - !!!Converting to unsigned 16-bit integer!!!")
        listings_raw[i] = listings_raw[i].astype('float').astype(pd.UInt16Dtype())
```
```{python}
#| echo: false
# Explanation: Row Cleaning 1 - Exclude listings with unrealistic prices.

count_price_expensive = (listings_raw['price'] > 2000).sum()
print(f"Number of rows with price greater than 2000: {count_price_expensive}")

count_price_zero = (listings_raw['price'] == 0).sum()
print(f"Number of rows with price equal to 0: {count_price_zero}")

count_price_missing = listings_raw['price'].isna().sum()
print(f"Number of rows with missing price: {count_price_missing}")

listings_raw = listings_raw[
    ~(  # Negate the combined conditions
        (listings_raw['price'] > 2000) |  
        (listings_raw['price'] == 0) |   
        (listings_raw['price'].isna())   
    )
]

count_price_expensive = (listings_raw['price'] > 2000).sum()
print(f"Number of rows with price greater than 2000 after cleaning: {count_price_expensive}")

count_price_zero = (listings_raw['price'] == 0).sum()
print(f"Number of rows with price equal to 0 after cleaning: {count_price_zero}")

count_price_missing = listings_raw['price'].isna().sum()
print(f"Number of rows with missing price after cleaning: {count_price_missing}")
```
```{python}
#| echo: false
# Explanation: Row Cleaning 2 - Calculate the number of missing values per row and plot a histogram of the distribution

probs = listings_raw.isnull().sum(axis=1) 
print(type(probs)) 
probs.plot.hist(bins=30)
```
```{python}
#| echo: false
#Explanation: Row Cleaning 2 - Drop rows where there are 10 or more missing values

cutoff = 10
listings_raw.drop(probs[probs > cutoff].index, inplace=True)
print(f"Have reduced data frame to: {listings_raw.shape[0]:,} rows and {listings_raw.shape[1]:,} columns")
```
```{python}
#| echo: false
#Explanation: Row Cleaning 3 - Drop rows where 'last_review' = Null

dropped_rows = len(listings_raw) - len(listings_raw.dropna(subset=['last_review']))
listings_raw = listings_raw.dropna(subset=['last_review'])
print(f"Number of rows dropped: {dropped_rows}")
```
```{python}
#| echo: false
#Explanation: Data cleaning validation (Seeded Rows)

listings_raw.sample(10, random_state=42)
```
```{python}
#| echo: false
#Explanation: Data cleaning validation (Column Type)

listings_clean = listings_raw
listings_clean.info()
```
```{python}
#| echo: false
#Explanation: Save the clean data locally

path = os.path.join('Data','Clean')

if not os.path.exists(path):
    print(f"Creating {path} under {os.getcwd()}") 
    os.makedirs(path)
    
listings_clean.to_csv(os.path.join(path,fn), index=False) 
print("Done.")
```
# Section C. Question 6
## EDA1: Are ‘Entire home/apt’ Properties Significantly More Than Others in London?
As of December 2023, data from Inside Airbnb indicates that there are approximately 66,000 active listings in London. These listings are categorized into types such as Entire home/apartment, Private room, Shared room, and Hotel room, with Entire home/apartment listings having the most significant impact on local rental markets. These properties are often removed from the long-term rental (LTR) market to serve as short-term rentals (STRs), which reduces housing supply and drives up rents. In London, approximately 43,000 listings are Entire home/apartment, making up about 64.01% of all Airbnb listings. This represents a notable share compared to other room types, as shown in the bar chart below (Fig 1), which compares the counts of each listing type. Additionally, 40.09% of these hosts have more than one listing, indicating a commercial and speculative nature of Airbnb lettings in London that may disrupt the local LTR market.
```{python}
#| echo: false
#Explanation: Calculate the total number of listings in the cleaned data frame

num_rows = listings_clean.shape[0]
print(f"Number of rows: {num_rows}")
```
```{python}
#| echo: false
#Explanation: Calculate the count of listings per room type & share of 'Entire home/apt'

room_type_counts = listings_clean['room_type'].value_counts()

print(room_type_counts)

# Count the total number of listings for each room type
room_type_counts = listings_raw['room_type'].value_counts()

# Calculate the proportion of 'Entire home/apt' relative to the total
entire_home_proportion = room_type_counts['Entire home/apt'] / room_type_counts.sum()

print(f"Proportion of 'Entire home/apt' listings: {entire_home_proportion:.4f}")
```
```{python}
#| echo: false
#Explanation: Assuming uniform distribution, this code performs a chi-squared test to confirm the count of 'Entire home/apt' is significantly higher than expected.

# Observed values
observed = room_type_counts.values

# Null hypothesis: The distribution of property types aligns with an equal distribution.
expected = [sum(observed) / len(observed)] * len(observed)

# Chi-square test
chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)

# Print
print(f"Chi-squared Statistic: {chi2_stat}")
print(f"P-value: {p_value}")

# Result
if p_value < 0.05:
    print("Reject the null hypothesis: 'Entire home/apt' is significantly higher than expected.")
else:
    print("Fail to reject the null hypothesis: The distribution matches the expected values.")
    #| echo: false
#Explanation: Plot the bar chart of the distribution of listings by room type.

from bokeh.io import output_notebook, show
from bokeh.plotting import figure
from bokeh.models import ColumnDataSource, HoverTool
from bokeh.palettes import Spectral4

# Assuming room_type_counts and expected are available in your environment
room_types = room_type_counts.index.to_list()
counts = room_type_counts.values.tolist()

# Create the ColumnDataSource for Bokeh
source = ColumnDataSource(data=dict(
    rt=room_types,
    count=counts,
))

# Add hover tool with custom tooltips
TOOLTIPS = [
    ("Room Type", "@rt"),
    ("Number of Listings", "@count{,}"),
]

# Create the Bokeh figure
p = figure(x_range=room_types, height=400, tooltips=TOOLTIPS, 
           title="Fig 1. Count of Airbnb Listings by Room Type in London (Up to December 10, 2023)",
           toolbar_location=None)

# Create the bar chart with custom color and tooltips
p.vbar(x='rt', top='count', width=0.9, source=source, color='purple')

# Add a dashed line for the mean (expected) value
p.line([-0.5, len(room_types)-0.5], [expected[0], expected[0]], 
       line_width=2, color="black", line_dash="dashed", legend_label="Mean")

# Customize the plot appearance
p.xgrid.grid_line_color = None
p.y_range.start = 0
p.legend.location = "top_right"
p.legend.orientation = "horizontal"
p.xaxis.axis_label = "Room Type"
p.yaxis.axis_label = "Count"

# Show the plot
output_notebook()  # Output to Jupyter notebook
show(p)  # Display the plot
```
```{python}
#| echo: false
#Explanation: Calculate the proportion of 'Entire home/apt' hosts as multiple-listing hosts

entire_home_apt = listings_clean[listings_clean['room_type'] == 'Entire home/apt']
more_than_1_entire = entire_home_apt[entire_home_apt['calculated_host_listings_count'] != 1]

entire_total_hosts = len(entire_home_apt['host_name'].unique())
entire_more_than_1_hosts = len(more_than_1_entire['host_name'].unique())
entire_ratio = entire_more_than_1_hosts / entire_total_hosts

print(f"Entire home/apt multiple listing host ratio: {entire_ratio:.2%}")
```

## EDA2: How Many of the 'Entire Home/apt' Hosts Are Not Following the 90-day Rules? 
Although London's [90-day rule](https://www.london.gov.uk/programmes-strategies/housing-and-land/improving-private-rented-sector/guidance-short-term-and-holiday-lets-london) restricts short-term rentals of entire homes to 90 nights per calendar year, enforcement remains weak. The histogram below (Fig. 2) shows the distribution of available days for entire home/apartment listings in London in the next calendar year (starting from December 10, 2023). 65.23% of these listings exceed the 90-day limit. The box plot (Fig. 3) shows that the median number of available days in the next calendar year is 167 days, with the maximum availability being for the entire year. These figures showcase widespread non-compliance with the 90-day rule among entire home/apartment hosts. One contributing factor is that fewer than [5%](https://airbtics.com/airbnb-rules-in-greater-london-united-kingdom/) of Airbnb listings in London display a registration number, meaning most hosts operate without regulation. In contrast, cities like [San Francisco](https://hosttools.com/blog/short-term-rental-tips/airbnb-rules-san-francisco/) have stricter measures, including data-sharing agreements with Airbnb to monitor host compliance. This highlights the regulatory gaps in London's STR market.
```{python}
#| echo: false
#Explanation: Create the histogram showing the distribution of 'availability_365' for 'Entire home/apt' in London

from bokeh.models import ColumnDataSource, Label

# Assuming entire_home_apt is available in your environment
availability_365_data = entire_home_apt[entire_home_apt['availability_365'] > 0]['availability_365']

# Calculate the histogram data
hist, edges = np.histogram(availability_365_data, bins=30)

# Create a ColumnDataSource for Bokeh
source = ColumnDataSource(data=dict(
    top=hist,
    left=edges[:-1],  # left edge of each bin
    right=edges[1:],  # right edge of each bin
))

# Calculate the percentage of data beyond 90
count_beyond_90 = (availability_365_data > 90).sum()
valid_rows = len(availability_365_data)
percentage_beyond_90 = (count_beyond_90 / valid_rows) * 100

# Create the Bokeh figure
p = figure(title="Fig 2. Distribution of Available Days for Entire Home/Apt in London",
           x_axis_label="Available Days in the Next Calendar Year from December 10, 2023",
           y_axis_label="Frequency",
           tools="pan,box_zoom,reset,hover")

# Create the histogram bars (quad creates the bars for histograms)
p.quad(top='top', bottom=0, left='left', right='right', source=source, color='purple', alpha=0.7)

# Add a vertical line at 90 days
p.line([90, 90], [0, max(hist)], line_width=2, color="black", line_dash="dashed", legend_label="90 Days")

# Add annotation for percentage of data beyond 90 days
label = Label(x=95, y=max(hist) * 0.8, text=f'{percentage_beyond_90:.2f}% above 90 days', 
              text_font_size="12px", text_color="black")
p.add_layout(label)

# Customize grid and axis
p.xgrid.grid_line_color = None
p.y_range.start = 0

# Show the plot
output_notebook()  # Output to Jupyter notebook
show(p)  # Display the plot
```
```{python}
#| echo: false
#Explanation: Create the box plot showing the distribution of 'availability_365' for 'Entire home/apt' in London

filtered_data_1 = entire_home_apt[entire_home_apt['availability_365'] > 0]

# Calculate the quartile values using pandas describe
stats = filtered_data_1['availability_365'].describe()

# Extract the quartile values
q1 = stats['25%']  # 25th percentile (Q1)
q2 = stats['50%']  # 50th percentile (Q2, Median)
q3 = stats['75%']  # 75th percentile (Q3)
min_value = stats['min']  # Minimum value
max_value = stats['max']  # Maximum value

# Create the Bokeh figure
p = figure(title="Fig 3. Box Plot of Available Days for Entire Home/Apt in London (Dec 2023 - Dec 2024)",
           x_axis_label="Available Days in the Next Calendar Year from December 10, 2023",
           tools="pan,box_zoom,reset",
           height=400, width=600)

# Add the box plot elements: Whiskers, Box, and Median

# Whiskers (extend from min to Q1 and Q3 to max)
p.segment(x0=min_value, y0=0.4, x1=min_value, y1=0.6, line_width=2, color="purple")
p.segment(x0=max_value, y0=0.4, x1=max_value, y1=0.6, line_width=2, color="purple")

# Box: From Q1 to Q3 (with height representing box thickness)
p.rect(x=(q1 + q3) / 2, y=0.5, width=q3 - q1, height=0.3, color="purple", alpha=0.5)

# Horizontal line connecting whiskers and box at the edges
p.line([min_value, q1], [0.5, 0.5], line_width=2, color="purple")  # Connecting min_value to Q1
p.line([q3, max_value], [0.5, 0.5], line_width=2, color="purple")  # Connecting Q3 to max_value

# Median Line at Q2
p.line([q2, q2], [0, 0.8], line_width=2, color="black", line_dash="dashed")

# Add label for the median (move it lower within plot range)
p.add_layout(Label(x=q2, y=0.8, text=f'Median = {int(q2)}', text_font_size="12px", text_color="black", text_baseline="middle", x_offset=10))

# Customize the plot appearance
p.ygrid.grid_line_color = None  # Remove y-axis grid lines for clarity
p.xaxis.major_label_orientation = 1  # Rotate x-axis labels if needed

# Display the plot
output_notebook()  # Output to Jupyter notebook
show(p)  # Show the plot
```
This section highlights the speculative and unregulated nature of entire home/apartment Airbnb listings in London, where weak enforcement of existing regulations may encourage speculative behavior. This raises the question of what could happen if the “90-day” rule were strictly enforced. The next section will explore this possibility in more detail.
## EDA3: Spatial Distribution of 'Entire Home/apt' Hosts Not Following the 90-day Rules
I am considering moving this part to Question 7 because there are only 15 points for this question (375 words if the total word count is 2500), and three plots have been included for it.
```{python}
#| echo: false
#Explanation: Loading London Borough boundaries (polygon)
import geopandas as gpd
ddir = os.path.join('data','geo') 
spath = 'https://github.com/jreades/fsds/blob/master/data/src/' 
boros = gpd.read_file(cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )
print('Done!')
boros.to_file('test.gpkg', driver='GPKG')
print(str(boros.geometry.iloc[1])[:50] + "...")
print(boros.crs)
```
```{python}
#| echo: false
#Explanation: Loading Airbnb listings (point)

gdf = gpd.GeoDataFrame(listings_clean,
                       geometry=gpd.points_from_xy(listings_clean.longitude, listings_clean.latitude, crs='epsg:4326'))
print(type(gdf))
print(gdf.geometry.iloc[1])
gdf = gdf.to_crs('epsg:27700') 
print(gdf.geometry.crs) #Reprojection
```
```{python}
#| echo: false
#Explanation: Subsetting the listings (points) to 'Entire home/apt' with 'availability_365' larger than 90.

entire_home_apt_90 = gdf[(gdf['room_type'] == 'Entire home/apt') & 
                          (gdf['availability_365'] > 90) & 
                          (gdf['availability_365'].notna())]
```
```{python}
#| echo: false
#Explanation: Mapping the spatial distribution of Entire home/apt with more than 90 days available in the next calendar year.

import geopandas as gpd
from bokeh.plotting import figure, show
from bokeh.models import GeoJSONDataSource, HoverTool
import json

# Reproject the GeoDataFrames to EPSG:4326 (WGS84, lat/lon)
boros = boros.to_crs('EPSG:4326')  # Reproject boroughs to WGS84 (lat/lon)
entire_home_apt_90 = entire_home_apt_90.to_crs('EPSG:4326')  # Reproject points to WGS84 (lat/lon)

# Convert GeoDataFrames into GeoJSONDataSource
def get_geodatasource(gdf):    
    """Convert a GeoDataFrame into GeoJSONDataSource for Bokeh"""
    json_data = json.dumps(json.loads(gdf.to_json()))
    return GeoJSONDataSource(geojson=json_data)

# Prepare the data for GeoJSON plotting
boros_geojson = get_geodatasource(boros)  # GeoJSON for borough polygons
entire_home_apt_90_geojson = get_geodatasource(entire_home_apt_90)  # GeoJSON for points

# Create a Bokeh figure
p = figure(title='Map 1. Entire Home/Apt Listings With More Than 90 Available Days in London (2023-2024)',
           height=700, width=850, toolbar_location='right', tools='wheel_zoom,pan,reset,hover')

# Plot the borough polygons (no map background)
p.patches('xs', 'ys', source=boros_geojson, fill_alpha=0.5, line_width=0.5, line_color='black', fill_color='grey')

# Plot the 'Entire home/apt' points with availability > 90
p.scatter(x='longitude', y='latitude', source=entire_home_apt_90_geojson, size=2, color='purple', legend_label='Entire home/apt > 90 days')

# Add hover tool for the points
hover = p.select(dict(type=HoverTool))
hover.tooltips = [("Available Days", "@availability_365")]

# Add labels, title, and legend
p.title.text_font_size = '16pt'
p.xaxis.axis_label = 'Longitude'
p.yaxis.axis_label = 'Latitude'
p.legend.title = 'Listings'

# Show the plot
show(p, notebook_handle=True)
```
::: {.duedate}

( 15 points; Answer due {{< var assess.group-date >}} )

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

( 45 points; Answer due {{< var assess.group-date >}} )

```{python}
#| echo: false
print(boros.columns)
```
```{python}
#| echo: false
# Perform a spatial join to associate each point with its corresponding 'GSS_CODE' in 'boros'
joined = gpd.sjoin(entire_home_apt_90, boros, how='left', predicate='within')

# Group by 'GSS_CODE' and count the number of points in each group
gss_code_counts = joined.groupby('GSS_CODE').size().reset_index(name='no_entire_home_apt_90')

# Merge the count back to the 'boros' GeoDataFrame
boros = boros.merge(gss_code_counts, on='GSS_CODE', how='left')

# Now 'boros' contains the new column 'no_entire_home_apt_90' with the count of points
```
```{python}
#| echo: false
print(boros.head())  # Display the first 5 rows
```
```{python}
#| echo: false
#Control 1: Population Density
url1 = "https://data.london.gov.uk/download/land-area-and-population-density-ward-and-borough/77e9257d-ad9d-47aa-aeed-59a00741f301/housing-density-borough.csv"

boros_population_density = pd.read_csv(cache_data(url1, os.path.join('Data', 'Independent variables')))

boros_population_density.info(verbose=True)
```
```{python}
#| echo: false
#Filter boros_population_density for the year 2023 and select relevant columns
boros_population_density_filtered = (
    boros_population_density
    .loc[boros_population_density['Year'] == 2023, 
         ['Code', 'Population', 'Population_per_square_kilometre']]
    .reset_index(drop=True)
)

#Drop the 'Code' column from 'boros' if it exists to avoid conflicts
if 'Code' in boros.columns:
    boros = boros.drop(columns=['Code'])

#Perform a left join with unique suffixes to handle duplicate columns
boros = boros.merge(
    boros_population_density_filtered, 
    left_on='GSS_CODE', 
    right_on='Code', 
    how='left', 
    suffixes=('', '_density')
)

#Drop the 'Code' column from the merged DataFrame if it exists
if 'Code' in boros.columns:
    boros = boros.drop(columns=['Code'])

#Reset the index of the final DataFrame
boros = boros.reset_index(drop=True)
#Create a new column (Dependent variable) by using 'no_entire_home_apt_90' divided by population, times 1000
boros['rate_entire_home_apt_90'] = (boros['no_entire_home_apt_90'] / boros['Population']) * 1000

#Display the first few rows of the final DataFrame
print(boros.head())
```
```{python}
#| echo: false
#Control 2: Median Taypayer Income
url2 = "https://data.london.gov.uk/download/average-income-tax-payers-borough/392e86d4-f1d3-4f06-a6a5-7fcd0fd65948/income-of-tax-payers.xlsx"
boros_income_raw = pd.read_excel(cache_data(url2, os.path.join('Data', 'Independent variables')), 
                                 sheet_name='Total Income', header=[0, 1])

# Step 2: Inspect the column names to understand the structure
print(boros_income_raw.columns)

# Step 3: Extract the 'Code' column (second row, first level entry)
boros_income_code = boros_income_raw[('Unnamed: 0_level_0', 'Code')]

# Step 4: Extract the 'Median £' column under '2021-22' header (second row, second level)
boros_income_median = boros_income_raw[('2021-22', 'Median £')]

# Step 5: Combine the 'Code' and 'Median £' columns into a new DataFrame
boros_income = pd.DataFrame({
    'Code': boros_income_code,
    'Median £': boros_income_median
})

# Step 6: Reset index for clarity
boros_income = boros_income.reset_index(drop=True)

# Step 7: Perform the left join with the 'boros' DataFrame
# Assuming 'boros' is already loaded and contains the 'GSS_CODE' column
boros = boros.merge(boros_income, left_on='GSS_CODE', right_on='Code', how='left')

# Step 8: Inspect the resulting DataFrame
print(boros.head())
```
```{python}
#| echo: false
#Control 3: Unemployment Rate
# URL and local path setup
url4 = "https://www.ethnicity-facts-figures.service.gov.uk/work-pay-and-benefits/unemployment-and-economic-inactivity/unemployment/latest/downloads/unemployment-by-local-authorities.csv"
local_path4 = os.path.join('Data', 'unemployment-by-local-authorities.csv')

# Read the unemployment data CSV file
unemployment_data_raw = pd.read_csv(cache_data(url4, local_path4))

# Filter rows where 'time' column equals 'Jan2022-Dec2022' and 'ethnicity' column equals 'All'
unemployment_data_filtered = unemployment_data_raw[
    (unemployment_data_raw['time'] == 'Jan2022-Dec2022') &
    (unemployment_data_raw['ethnicity'] == 'All') &
    (unemployment_data_raw['ethnicity_type'] == 'All')
]

# Keep only 'Geography' and 'value' columns
unemployment_data_filtered = unemployment_data_filtered[['Geography', 'value']]

# Perform left join with the existing `boros` DataFrame
boros = boros.merge(
    unemployment_data_filtered,
    how='left',
    left_on='NAME',
    right_on='Geography'
)
# Rename 'value' column to 'unemployment_rate'
boros.rename(columns={'value': 'unemployment_rate'}, inplace=True)

# Drop 'Geography' column to reduce redundancy
boros.drop(columns=['Geography'], inplace=True)

# Display the updated `boros` DataFrame
print(boros)
```
```{python}
#| echo: false
#Control 4: Rented Housing Demand (% as renters)
# Step 1: URL and local path setup
url5 = "https://data.london.gov.uk/download/housing-tenure-borough/53cb03c2-3e99-4d71-937b-90b95a64ebaa/tenure-population-borough.csv"
local_path5 = os.path.join('Data', 'housing-tenure-by-local-authorities.csv')

# Read the tenure data CSV file
tenure_data_raw = pd.read_csv(cache_data(url5, local_path5))

# Ensure 'Percent_of_population_in_borough' is numeric
tenure_data_raw['Percent_of_population_in_borough'] = pd.to_numeric(
    tenure_data_raw['Percent_of_population_in_borough'], errors='coerce'
)

# Step 2: Select rows for the specified conditions
tenure_filtered = tenure_data_raw[
    (tenure_data_raw['Year'] == 2018) & 
    (tenure_data_raw['Tenure'].isin([
        'Rented.from.Local.Authority.or.Housing.Association',
        'Rented.from.Private.landlord'
    ]))
]
# Step 3: Create a pivot table for individual values and calculate the sum
tenure_pivot = tenure_filtered.pivot_table(
    index='Code', 
    columns='Tenure', 
    values='Percent_of_population_in_borough',
    aggfunc='sum'  # Use 'sum' to handle potential duplicate rows
).reset_index()

# Rename the columns for clarity
tenure_pivot.columns.name = None  # Remove the MultiIndex
tenure_pivot.rename(columns={
    'Rented.from.Local.Authority.or.Housing.Association': 'percent_local_authority_housing',
    'Rented.from.Private.landlord': 'percent_private_landlord'
}, inplace=True)

# Calculate the sum for the two categories
tenure_pivot['percent_of_renters'] = (
    tenure_pivot['percent_local_authority_housing'] + tenure_pivot['percent_private_landlord']
)

# Step 4: Final DataFrame with all required columns
boros_renters = tenure_pivot[['Code', 'percent_local_authority_housing', 'percent_private_landlord', 'percent_of_renters']]

# Display the resulting DataFrame
print(boros_renters.head())
```
```{python}
#| echo: false
# Perform the left join between 'boros' and 'boros_renters'
boros = boros.merge(
    boros_renters[['Code', 'percent_of_renters']],  # Select only the 'Code' and 'percent_of_renters' columns
    how='left',  # Left join
    left_on='GSS_CODE',  # Field from the left object
    right_on='Code'  # Field from the right object
)

# Display the updated DataFrame
print(boros.head())
```
```{python}
#| echo: false
#Control 5: Rented Housing Supply (Housing stock per person)
# Step 1: URL and local path setup
url6 = "https://data.london.gov.uk/download/local-authority-housing-stock/3421002e-f0ec-45e5-bf73-f594788ec93d/local-authority-housing-stock-borough.csv"
local_path6 = os.path.join('Data', 'housing-stock-by-local-authorities.csv')

# Read the housing stock data CSV file
housing_stock_data_raw = pd.read_csv(cache_data(url6, local_path6))

# Step 2: Select the 'Number of properties 2020' and 'Code' columns
boros_housing_stock = housing_stock_data_raw[['Number of properties 2020', 'Code']].copy()

# Step 3: Remove commas and convert 'Number of properties 2020' to numeric
boros_housing_stock['Number of properties 2020'] = boros_housing_stock['Number of properties 2020'].str.replace(',', '').astype(float)

# Step 4: Rename the column 'Number of properties 2020' to 'housing_stock_2020'
boros_housing_stock.rename(columns={'Number of properties 2020': 'housing_stock_2020'}, inplace=True)

# Step 5: Ensure the new DataFrame is named correctly
boros_housing_stock.reset_index(drop=True, inplace=True)

# Display the resulting DataFrame
print(boros_housing_stock.head())
# Step 1: URL and local path setup
url6 = "https://data.london.gov.uk/download/local-authority-housing-stock/3421002e-f0ec-45e5-bf73-f594788ec93d/local-authority-housing-stock-borough.csv"
local_path6 = os.path.join('Data', 'housing-stock-by-local-authorities.csv')

# Read the housing stock data CSV file
housing_stock_data_raw = pd.read_csv(cache_data(url6, local_path6))

# Step 2: Select the 'Number of properties 2020' and 'Code' columns
boros_housing_stock = housing_stock_data_raw[['Number of properties 2020', 'Code']].copy()

# Step 3: Remove commas and convert 'Number of properties 2020' to numeric
boros_housing_stock['Number of properties 2020'] = boros_housing_stock['Number of properties 2020'].str.replace(',', '').astype(float)

# Step 4: Rename the column 'Number of properties 2020' to 'housing_stock_2020'
boros_housing_stock.rename(columns={'Number of properties 2020': 'housing_stock_2020'}, inplace=True)

# Step 5: Ensure the new DataFrame is named correctly
boros_housing_stock.reset_index(drop=True, inplace=True)

# Display the resulting DataFrame
print(boros_housing_stock.head())
```
```{python}
#| echo: false
# Perform the left join between 'boros' and 'boros_housing_stock'
boros = boros.merge(
    boros_housing_stock[['Code', 'housing_stock_2020']],  # Select only necessary columns
    how='left',
    left_on='GSS_CODE',
    right_on='Code',
    suffixes=('', '_housing_stock')  # Specify suffixes to avoid column name conflicts
)

# Drop the 'Code' column after the merge to avoid redundancy
if 'Code' in boros.columns:
    boros = boros.drop(columns=['Code'])

# Create the new column 'number_of_dwellings_per_person'
boros['number_of_dwellings_per_person'] = boros['housing_stock_2020'] / boros['Population']

# Display the updated DataFrame
print(boros.head())
```
```{python}
#| echo: false
#Control 6: Education Amenity (Average attainment 8 score per pupil)
# Step 1: URL and local path setup
url7 = "https://data.london.gov.uk/download/gcse-results-by-borough/a8a71d73-cc48-4b30-9eb5-c5f605bc845c/GCSE%20results%20by%20gender.xlsx"
local_path7 = os.path.join('Data', 'GCSE_results_by_gender.xlsx')

# Download and save the file using the cache function
file_path7 = cache_data(url7, local_path7)

# Step 2: Load the '2022-23' sheet and skip the metadata rows
data = pd.read_excel(file_path7, sheet_name='2022-23', skiprows=4)

# Step 3: Rename columns for clarity
data.rename(columns={
    'E09000001': 'Code',
    'Unnamed: 3': 'Average Attainment 8 score per pupil'
}, inplace=True)

# Step 4: Select the relevant columns
boros_education = data[['Code', 'Average Attainment 8 score per pupil']].copy()

# Step 5: Drop rows where 'Code' is missing to clean the data
boros_education.dropna(subset=['Code'], inplace=True)

# Step 6: Reset the index for the final DataFrame
boros_education.reset_index(drop=True, inplace=True)

# Step 7: Display the resulting DataFrame
print(boros_education.head())
```
```{python}
#| echo: false
# Perform the left join
boros = boros.merge(
    boros_education,  # Right object
    how='left',  # Left join
    left_on='GSS_CODE',  # Field in the left object
    right_on='Code'  # Field in the right object
)

# Drop the 'Code' column from the resulting DataFrame to avoid redundancy
if 'Code' in boros.columns:
    boros.drop(columns=['Code'], inplace=True)

# Display the updated DataFrame
print(boros.head())
```
```{python}
#| echo: false
#Control 7: Public Transport (Public transport accessibility level)
# Step 1: URL and local path setup
url8 = "https://data.london.gov.uk/download/public-transport-accessibility-levels/8e520b81-dd06-4ce6-aaa0-972dccf84b57/Borough%20AvPTAI2015.csv"
local_path8 = os.path.join('Data', 'Borough_AvPTAI2015.csv')

# Cache and save the file locally
file_path8 = cache_data(url8, local_path8)

# Step 2: Load the data and extract the relevant columns
boros_PTA = pd.read_csv(file_path8)[['Borough Code', 'AvPTAI2015']].copy()

# Step 3: Perform the left join
boros = boros.merge(
    boros_PTA,  # Right object
    how='left',  # Left join
    left_on='GSS_CODE',  # Left field
    right_on='Borough Code'  # Right field
)

# Drop the 'Borough Code' column after the join to avoid redundancy
if 'Borough Code' in boros.columns:
    boros.drop(columns=['Borough Code'], inplace=True)

# Display the updated DataFrame
print(boros.head())
```
```{python}
#| echo: false
#Control 8: Green space (% of land as green and blue infrastructure)
# Step 1: URL and local path setup
url9 = "https://data.london.gov.uk/download/green-and-blue-cover/fdff7445-bb87-4584-ada8-d527e7fcec97/green_cover_borough_summary_0.05.xlsx"
local_path9 = os.path.join('Data', 'green_cover_borough_summary_0.05.xlsx')

# Cache and save the file locally
file_path9 = cache_data(url9, local_path9)

# Step 2: Load the 'borough_green_cover' sheet and extract relevant columns
boros_green_blue = pd.read_excel(file_path9, sheet_name='borough_green_cover')[['lb_name', 'percent_green+blue']].copy()

# Step 3: Perform the left join
boros = boros.merge(
    boros_green_blue,  # Right object
    how='left',  # Left join
    left_on='GSS_CODE',  # Left field
    right_on='lb_name'  # Right field
)

# Drop the 'lb_name' column after the join to avoid redundancy
if 'lb_name' in boros.columns:
    boros.drop(columns=['lb_name'], inplace=True)

# Display the updated DataFrame
print(boros.head())
```
```{python}
#| echo: false
# Dependent Variable: Mean rent (per bedroom)
# Define the cache_data function to always re-download the file
def cache_data(url, directory):
    os.makedirs(directory, exist_ok=True)
    file_path = os.path.join(directory, os.path.basename(url))
    response = requests.get(url)
    response.raise_for_status()
    with open(file_path, 'wb') as file:
        file.write(response.content)
    return file_path

# Download and load the file
url3 = "https://www.ons.gov.uk/file?uri=/economy/inflationandpriceindices/adhocs/2417privaterentalmarketinlondonoctober2023toseptember2024/londonrentalstatsaccessibleq32024.xlsx"
file_path3 = cache_data(url3, os.path.join('Data', 'Independent variables'))

# Inspect sheet names and load the desired sheet
sheet_names = pd.ExcelFile(file_path3).sheet_names
print("Sheet names:", sheet_names)
sheet_data = pd.read_excel(file_path3, sheet_name='2', skiprows=2) if '2' in sheet_names else pd.read_excel(file_path3, sheet_name=4, skiprows=2)

# Display the first few rows of the loaded sheet
print(sheet_data.head())

# Load the sheet '2' with skiprows=2
sheet_data = pd.read_excel(file_path3, sheet_name='2', skiprows=2)

# Replace non-numeric values in 'Mean' column with NaN
sheet_data['Mean'] = pd.to_numeric(sheet_data['Mean'], errors='coerce')

# Loop through each borough to calculate mean_rent
results = []

for borough in sheet_data['Borough'].unique():
    borough_data = sheet_data[sheet_data['Borough'] == borough]
    
    # Extract mean values for each category
    studio_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Studio', 'Mean'].dropna().values
    one_bedroom_mean = borough_data.loc[borough_data['Bedroom Category'] == 'One Bedroom', 'Mean'].dropna().values
    two_bedrooms_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Two Bedrooms', 'Mean'].dropna().values / 2
    three_bedrooms_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Three Bedrooms', 'Mean'].dropna().values / 3
    four_or_more_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Four or More Bedrooms', 'Mean'].dropna().values / 4

    # Calculate the mean_rent
    mean_rent = (
        (studio_mean[0] if len(studio_mean) > 0 else 0) +
        (one_bedroom_mean[0] if len(one_bedroom_mean) > 0 else 0) +
        (two_bedrooms_mean[0] if len(two_bedrooms_mean) > 0 else 0) +
        (three_bedrooms_mean[0] if len(three_bedrooms_mean) > 0 else 0) +
        (four_or_more_mean[0] if len(four_or_more_mean) > 0 else 0)
    ) / 5

# Append the result
    results.append({
        'Name': borough,
        'mean_rent': mean_rent
    })

# Create a DataFrame from the results
boros_rent = pd.DataFrame(results)

# Display the final DataFrame
print(boros_rent)
```
```{python}
#| echo: false
# Perform the left join
boros = boros.merge(
    boros_rent,  # Right object
    how='left',  # Left join
    left_on='NAME',  # Left field
    right_on='Name'  # Right field
)

# Drop the 'Borough' column after the join to avoid redundancy
if 'Name' in boros.columns:
    boros.drop(columns=['Name'], inplace=True)

# Display the updated DataFrame
print(boros.head())
```
```{python}
#| echo: false
print(regression_data.head())
```
```{python}
#| echo: false
#OLS Regression
regression_data.info()
```
```{python}
#| echo: false
# Replace non-numeric values in 'unemployment_rate' with NaN
regression_data['unemployment_rate'] = pd.to_numeric(regression_data['unemployment_rate'], errors='coerce')

# Convert 'inner_outer' to boolean: assume 0 represents False and all else represents True
regression_data['inner_outer'] = regression_data['inner_outer'].apply(lambda x: False if x == 1 else True)

# Display the updated DataFrame to verify changes
regression_data.info()
print(regression_data.head())
```
```{python}
#| echo: false
# Step 1: Identify rows with NaN values
rows_with_na = regression_data[regression_data.isnull().any(axis=1)]
if not rows_with_na.empty:
    print("Rows containing NaN values that will be dropped:")
    print(rows_with_na)

# Step 2: Drop rows with NaN values
regression_data = regression_data.dropna()

# Step 3: Display the cleaned DataFrame
print("Cleaned DataFrame:")
print(regression_data.head())
```
```{python}
#| echo: false
# Step 1. Libraries
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import pandas as pd
import statsmodels.api as sm

# Step 2. Define VIF Function (drop_column_using_vif_)
def drop_column_using_vif_(df, thresh=5): 
    while True:
        df_with_const = add_constant(df)
        vif_df = pd.Series(
            [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])],
            name="VIF",
            index=df_with_const.columns
        ).to_frame()
        vif_df = vif_df.drop('const')
        if vif_df.VIF.max() > thresh:
            index_to_drop = vif_df.index[vif_df.VIF == vif_df.VIF.max()].tolist()[0]
            print(f"Dropping: {index_to_drop}")
            df = df.drop(columns=index_to_drop)
        else: 
            break
    return df

# Step 3. Clean the data by ensuring numeric values and converting booleans
predictor_columns = ['mean_rent', 'GSS_CODE', 'HECTARES', 'NAME', 'geometry']
predictors = regression_data.drop(columns=predictor_columns)

# Convert boolean columns to integers (if any)
predictors = predictors.applymap(lambda x: int(x) if isinstance(x, bool) else x)

# Convert all predictors to numeric and coerce errors to NaN
predictors = predictors.apply(pd.to_numeric, errors='coerce')

# Identify rows with NaN values and print them
rows_with_na = predictors[predictors.isnull().any(axis=1)]
if not rows_with_na.empty:
    print("Rows containing NaN values that will be dropped:")
    print(rows_with_na)

# Drop rows with NaN values
predictors = predictors.dropna()

# Step 4. VIF Test on Variables
df_predictors_selected_VIF = drop_column_using_vif_(predictors)

print("The columns remaining after VIF selection are:")
print(df_predictors_selected_VIF.columns)  # Show the dropped and remaining variables

# Ensure the dependent variable is cleaned to match the predictors
endog = regression_data.loc[df_predictors_selected_VIF.index, 'mean_rent']

# Add a constant to predictors and perform OLS regression
exog = sm.add_constant(df_predictors_selected_VIF)  # Add constant term
OLS_regression_data = sm.OLS(endog=endog, exog=exog).fit()

# Print the regression summary
print(OLS_regression_data.summary())
```
:::

## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
