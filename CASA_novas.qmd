---
bibliography: CASA_novas_bib.bib
csl: harvard-cite-them-right.csl
title: CASA_novas's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto Flex
    monofont: Liberation Mono
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| echo: false
#Explanation: Import libraries
import os
import pandas as pd
```

```{python}
#| echo: false
#Explanation: This code chunk downloads a file from a given URL and saves it locally to a specified destination, caching it if it doesn't already exist or is too small, and returns the local file path.

from urllib.parse import urlparse
import os
import requests  

def cache_data(src: str, dest: str) -> str:
    """
    Downloads and caches a remote file locally.

    Parameters
    ----------
    src : str
        The remote *source* for the file, any valid URL should work.
    dest : str
        The *destination* location to save the downloaded file.
        
    Returns
    -------
    str
        A string representing the local location of the file.
    """
    url = urlparse(src)
    fn = os.path.split(url.path)[-1]
    dfn = os.path.join(dest, fn)

    if not os.path.isfile(dfn) or os.stat(dfn).st_size < 250:
        print(f"{dfn} not found, downloading!")

        # Create directories if they don't exist
        os.makedirs(dest, exist_ok=True)

        try:
            response = requests.get(src)
            response.raise_for_status()  

            with open(dfn, "wb") as file:
                file.write(response.content)

            print("\tDone downloading...")

            f_size = os.stat(dfn).st_size
            print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

        except requests.exceptions.RequestException as e:
            print(f"Failed to download file from {src}")
            raise e

    else:
        print(f"Found {dfn} locally!")
        f_size = os.stat(dfn).st_size
        print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

    return dfn
```
```{python}
#| echo: false
#Explanation: The code constructs a URL to access a specific dataset from Inside Airbnb, downloads and caches the file, reads it into a Pandas DataFrame, and then prints the shape and detailed information

host = 'https://data.insideairbnb.com'
country = 'united-kingdom'
state = 'england'
city = 'london'
ymd = '2024-09-06'
url = f'{host}/{country}/{state}/{city}/{ymd}/data/listings.csv.gz'

listings_raw = pd.read_csv(cache_data(url, os.path.join('data', 'raw')))

print(f"Data frame is {listings_raw.shape[0]:,} x {listings_raw.shape[1]}")
listings_raw.info(verbose=True)
```


## 1. Who collected the InsideAirbnb data?

::: {.duedate}

( 2 points; Answer due Week 7 )

The Inside Airbnb data was primarily collected by [Murray Cox](https://twitter.com/murrayscox), a Brooklyn based independent researcher who started the project in 2015. While Cox leads the initiative, a network of volunteers, activists, and researchers supports the data collection by providing local context, refining scraping tools, and organizing the raw data.

:::


## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}

( 4 points; Answer due Week 7 )

Inside Airbnb collected data to reveal the impact of short-term lets (STL) on residential communities, often obscured by Airbnb’s PR-driven data. In 2015, Airbnb purged over 1,000 listings from multi-listing hosts before presenting a manipulated snapshot under its ‘transparency initiative’ [@cox_how_2020]. If this scandal had not been exposed, Airbnb’s harm to local housing markets would have been downplayed. As a grassroots project, Inside Airbnb challenges dominant corporate narratives by providing transparent data that highlights Airbnb’s role in gentrification, eviction, and the housing crisis. This empowers researchers, policymakers, and marginalized communities to collectively advocate for fair STL regulations and social justice—exemplifying 'data for co-liberation' [@dignazio_data_2020].
:::


## 3. How did they collect it?

::: {.duedate}

( 5 points; Answer due Week 8 )

:::

## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

( 11 points; Answer due Week 9 )

:::

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}

( 18 points; Answer due {{< var assess.group-date >}} )

:::

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 

::: {.duedate}

( 15 points; Answer due {{< var assess.group-date >}} )

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

( 45 points; Answer due {{< var assess.group-date >}} )

:::

## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
