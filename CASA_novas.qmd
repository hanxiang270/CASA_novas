---
bibliography: CASA_novas_bib.bib
csl: harvard-cite-them-right.csl
title: CASA_novas's Group Project
execute:
  echo: false
  output: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto Flex
    monofont: Liberation Mono
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| results: hide
#Explanation: Import initial libraries
import os
import pandas as pd
import requests
```

```{python}
#Explanation: This code chunk is a simple function to download files from url, which will let us download the bib files.
def download_bib(url, out_path):
    """
    Downloads a remote file locally.
    
    Parameters
    ----------
    url : str
        The link of the file, any valid URL should work.
    out_path : str
        The *destination* location to save the downloaded file.
        
    """
    if not os.path.exists(out_path):
        try:
            response = requests.get(url)
            response.raise_for_status()
            with open(out_path, 'w', encoding='utf-8') as file:
                file.write(response.text)
        except requests.exceptions.RequestException as e:
            print(f"Failed to get document: {e}")

download_bib("https://raw.githubusercontent.com/hanxiang270/CASA_novas/refs/heads/main/CASA_novas_bib.bib", "CASA_novas_bib.bib")

download_bib("https://raw.githubusercontent.com/hanxiang270/CASA_novas/refs/heads/main/harvard-cite-them-right.csl", "harvard-cite-them-right.csl")
```

# Response to Questions

## 1. Who collected the InsideAirbnb data?


( 2 points; Answer due Week 7 )

The InsideAirbnb data was primarily collected by [Murray Cox](https://twitter.com/murrayscox), a Brooklyn-based independent researcher who started the project in 2015. While Cox leads the initiative, a network of activists and researchers supports the data collection by providing local context, refining scraping tools, and organizing the raw data.



## 2. Why did they collect the InsideAirbnb data?


( 4 points; Answer due Week 7 )

InsideAirbnb collected data to reveal the impact of short-term rentals (STR) on residential communities, often obscured by Airbnb’s PR-driven data. For instance, in 2015, Airbnb purged over 1,000 listings from multi-listing hosts before presenting a manipulated snapshot under its ‘transparency initiative’ [@cox_how_2020]. If this scandal had not been exposed, Airbnb’s harm to local housing markets would have been downplayed. As a grassroots project, InsideAirbnb aims to challenge this issue by providing transparent data that highlights Airbnb’s role in gentrification, eviction, and the housing crisis. This empowers researchers, policymakers, and marginalized communities to collectively advocate for fair STR regulations and social justice — exemplifying 'data for co-liberation' [@dignazio_data_2020].


## 3. How did they collect it?


( 5 points; Answer due Week 8 )

Regarding the methods of collection of InsideAirbnb, “the website only provides minimal information on the data collection process” [@Alsudais:2021]. From their [website](https://insideairbnb.com/data-assumptions/) we can see that they compile, in a quarterly basis, public information from the Airbnb website using python scripts for web-scraping. According to Alsudais, “these scripts included ones that have been “copied and pasted” from other online resources” [@Alsudais:2021]. Through this method of collection, they gather different data variables from each listing including numeric (e.g., host listings count, minimum nights, price per night), categorical (e.g., host ID), spatial (e.g.,  longitude and latitude), and textual (e.g., description) data, which is later cleansed and aggregated in files open to the public. 

## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?

( 11 points; Answer due Week 9 )

Although InsideAirbnb states that it is [not responsible for the accuracy of its data](https://insideairbnb.com/data-assumptions/), its automated scraping method is susceptible to “systemic errors” [@Alsudais:2021]. For instance, manual inspection has confirmed that the scraper incorrectly collects all reviews associated with a listing ID, regardless of whether the listing is categorized as a "place" (e.g., apartments) or an "experience" (e.g., tours). This issue may introduce bias into Natural Language Processing (NLP) studies analyzing reviews, potentially distorting insights into guest preferences and neighborhood characteristics. 

Also, InsideAirbnb data accuracy is compromised as the scraped information relies on Airbnb and host-generated content that may be inaccurate. For instance, Airbnb anonymizes location data, causing the scraped locations to appear scattered around the actual addresses. This creates challenges for local authorities attempting to regulate unregistered hosts in London [@ferreri_platform_2018] and introduces errors in studies analyzing Airbnb’s impact on specific neighborhoods.  

Furthermore, hosts may misrepresent listing characteristics, such as misclassifying room types as [Entire Home or Private Room](https://community.withairbnb.com/t5/Ask-about-your-listing/Entire-home-or-private-room-ANY-THOUGHTS/m-p/421087). This can lead to incorrect conclusions about the distribution of Airbnb listings across property types and their overall impact on housing availability and market dynamics.  

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

( 18 points)

While the InsideAirbnb data provides insights on the STR's effects, its use raises several ethical concerns, particularly around privacy, consent, rights to use the data, and bias. 

About privacy and consent, although the data is scraped from publicly available information on the Airbnb website, researchers using the data do not seek explicit consent from the individuals whose data is included. Even though hosts have accepted Airbnb’s Terms of Service to make their data public to offer their listings, they often are not able to understand the implications of their consent effectively [@solove_privacy_2012] and they didn’t give consent for other purposes like research [@scassa_ownership_2019]. Moreover, because of the “aggregation effect” [@solove_privacy_2012], combining seemingly simple data, represents risks of exposing sensitive information or personally identifying data, for example when combining photos from the listings with locations and personal details exposed on the reviews. 

On rights to use the data, Airbnb's Terms of Service prohibit the downloading or scraping of data from its site, but this obligation only binds the parties to the agreement and not third parties who use the data [@scassa_ownership_2019]. This raises questions about who owns and controls the data and whether researchers should be allowed to use it without explicit permission. 

Finally, bias is an ethical issue to consider when using the data for research purposes and evidence-based policymaking. While the InsideAirbnb data may appear unbiased, the way it is analysed and interpreted can introduce significant biases emerging from the diverse perspectives and methodologies of the institutions conducting the analysis, that may result on a reductionist way of assessing the phenomena. For example, [@Quattrone:2018] mentions that municipalities often approach Airbnb regulation through a “false dichotomy,” either allowing the platform to operate without restrictions or banning it entirely. Such reductionist approaches can disproportionately harm small-scale hosts who are not misusing the platform and rely on it to cover living costs, which according to [Airbnb News (2002)](https://news.airbnb.com/en-uk/third-of-hosts-use-airbnb-income-to-afford-rising-living-costs/), might represent up to a third of UK hosts.  

To avoid ethical issues arising from biased analysis, according to D’Ignazio [@dignazio_data_2020], it is important acknowledging that data is not raw, and analysis is not fully neutral and requires transparency about context and methods. Also, it’s necessary to keep in mind that behind numbers on the data there are people whose livelihoods may be directly impacted by policy decisions, so quantitative analysis is not enough to address this kind of phenomenon and must be complemented with qualitative research, consent from those whose data is used, and a critical approach of the data. 

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 


( 15 points)

```{python}
#Explanation: This code chunk defines a function that downloads a file from a given URL and saves it locally to a specified destination,
#caching it if it doesn't already exist or is too small,and returns the local file path. We will use this for downloading the data sets

from urllib.parse import urlparse
import requests  

def cache_data(src: str, dest: str) -> str:
    """
    Downloads and caches a remote file locally.

    Parameters
    ----------
    src : str
        The remote *source* for the file, any valid URL should work.
    dest : str
        The *destination* location to save the downloaded file.
        
    Returns
    -------
    str
        A string representing the local location of the file.
    """
    url = urlparse(src)
    fn = os.path.split(url.path)[-1]
    dfn = os.path.join(dest, fn)

    if not os.path.isfile(dfn) or os.stat(dfn).st_size < 250:
        print(f"{dfn} not found, downloading!")

        # Create directories if they don't exist
        os.makedirs(dest, exist_ok=True)

        try:
            response = requests.get(src)
            response.raise_for_status()  

            with open(dfn, "wb") as file:
                file.write(response.content)

            print("\tDone downloading...")

            f_size = os.stat(dfn).st_size
            print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

        except requests.exceptions.RequestException as e:
            print(f"Failed to download file from {src}")
            raise e

    else:
        print(f"Found {dfn} locally!")
        f_size = os.stat(dfn).st_size
        print(f"\tSize is {f_size / 1024**2:,.0f} MB ({f_size:,} bytes)")

    return dfn
```

```{python}
#Explanation: The code constructs a URL to access a specific dataset from Inside Airbnb, downloads and caches the file, reads it into a Pandas DataFrame, and then prints the shape and detailed information.

host = 'https://data.insideairbnb.com'
country = 'united-kingdom'
state = 'england'
city = 'london'
ymd = '2023-12-10'
url = f'{host}/{country}/{state}/{city}/{ymd}/data/listings.csv.gz'

listings_raw = pd.read_csv(cache_data(url, os.path.join('data', 'raw')))

print(f"Data frame is {listings_raw.shape[0]:,} x {listings_raw.shape[1]}")
listings_raw.info(verbose=True)
```

```{python}
#| results: hide
#Explanation: The code saves the dataset to a specified folder (Data/Raw) in the current working directory as a CSV file.

path = os.path.join('Data', 'Raw') 
fn = url.split('/')[-1] 
print(f"Writing to: {fn}")

if not os.path.exists(path):
    print(f"Creating {path} under {os.getcwd()}")
    os.makedirs(path)

if not os.path.exists(os.path.join(path, fn)): 
    listings_raw.to_csv(os.path.join(path, fn), index=False) 
    print("Done.")
```

```{python}
#Explanation: Import packages
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sms
import seaborn as sns
from scipy.stats import chisquare
```

```{python}
#Explanation: To check the data type for each column by a for loop.
for col in listings_raw.columns:
    print(f"{col}: {listings_raw[col].dtype}")
```

```{python}
#Explanation: Randomly selects 5 rows from the DataFrame listings_raw using a fixed random seed for reproducibility.

listings_raw.sample(10, random_state=42)

#Observation on columns:
#(1)Some columns contain predominantly null values, such as 'license' and 'description'.
#(2)The 'property_type' and 'room_type' column should be in categorical format, not object.
#(3)Some columns, such as 'price', should be in numeric format, not object.
#To ensure the accuracy and reliability of our analysis, columns with predominantly null values will be dropped. Some 'object' columns will be converted to categorical and numeric formats.

#Observation on rows: 
#(1)Some rows have unrealistic 'price'.
#(2)Some rows contain more columns with null values than others.
#(3)Some rows have null value in 'last_review' and a value of '0' in the 'availability_365' column, indicating that the property is not available.
#To ensure the accuracy and reliability of our analysis, these rows will be removed in the upcoming cleaning process.
```

```{python}
#Explanation: Column Cleaning 1 - Get the top 10 columns with the most null values

top_10_columns = listings_raw.isnull().sum(axis=0).sort_values(ascending=False)[:10].index

print("Number of null values in top 10 columns:")
print(listings_raw[top_10_columns].isnull().sum())
```

```{python}
#Explanation: Column Cleaning 1 - Drop the top 10 columns

listings_raw = listings_raw.drop(columns=top_10_columns)

print("\nNumber of null values in remaining columns after dropping the top 10 columns:")
print(listings_raw.isnull().sum().sort_values(ascending=False)[:10])
```

```{python}
#Explanation: Column Cleaning 2 - Convert 'room_type' and 'property_type' to categories.

cats = ['property_type','room_type']

for c in cats:
    print(f"Converting {c}")
    listings_raw[c] = listings_raw[c].astype('category')

print(f"Now {cats[0]} is of type '{listings_raw[cats[0]].dtype}'", "\n") 
print(listings_raw[cats[0]].cat.categories.values)

print(f"Now {cats[1]} is of type '{listings_raw[cats[1]].dtype}'", "\n") 
print(listings_raw[cats[1]].cat.categories.values)
```

```{python}
#Explanation: Column Cleaning 3 - Convert 'price' to numeric format.

money = ['price']

for m in money:
    print(f"Converting {m}")
    listings_raw[m] = listings_raw[m].str.replace('$','', regex=False).str.replace(',','').astype('float')

listings_raw.sample(10, random_state=42) [money]
```

```{python}
# Explanation: Column Cleaning 4 - Convert other object columns to integer format.

ints = ['id', 'host_id', 'host_listings_count', 'host_total_listings_count', 'accommodates', 
        'beds', 'minimum_nights', 'maximum_nights', 'availability_365']

for i in ints:
    print(f"Converting {i}")
    try:
        listings_raw[i] = listings_raw[i].astype('float').astype('int')
    except ValueError as e:
        print(" - !!!Converting to unsigned 16-bit integer!!!")
        listings_raw[i] = listings_raw[i].astype('float').astype(pd.UInt16Dtype())
```

```{python}
# Explanation: Row Cleaning 1 - Exclude listings with unrealistic prices.

count_price_expensive = (listings_raw['price'] > 2000).sum()
print(f"Number of rows with price greater than 2000: {count_price_expensive}")

count_price_zero = (listings_raw['price'] == 0).sum()
print(f"Number of rows with price equal to 0: {count_price_zero}")

count_price_missing = listings_raw['price'].isna().sum()
print(f"Number of rows with missing price: {count_price_missing}")

listings_raw = listings_raw[
    ~(  # Negate the combined conditions
        (listings_raw['price'] > 2000) |  
        (listings_raw['price'] == 0) |   
        (listings_raw['price'].isna())   
    )
]

count_price_expensive = (listings_raw['price'] > 2000).sum()
print(f"Number of rows with price greater than 2000 after cleaning: {count_price_expensive}")

count_price_zero = (listings_raw['price'] == 0).sum()
print(f"Number of rows with price equal to 0 after cleaning: {count_price_zero}")

count_price_missing = listings_raw['price'].isna().sum()
print(f"Number of rows with missing price after cleaning: {count_price_missing}")
```

```{python}
# Explanation: Row Cleaning 2 - Calculate the number of missing values per row and plot a histogram of the distribution

probs = listings_raw.isnull().sum(axis=1) 
print(type(probs)) 
probs.plot.hist(bins=30)
```

```{python}
#Explanation: Row Cleaning 2 - Drop rows where there are 10 or more missing values

cutoff = 10
listings_raw.drop(probs[probs > cutoff].index, inplace=True)
print(f"Have reduced data frame to: {listings_raw.shape[0]:,} rows and {listings_raw.shape[1]:,} columns")
```

```{python}
#Explanation: Row Cleaning 3 - Drop rows where 'last_review' = Null

dropped_rows = len(listings_raw) - len(listings_raw.dropna(subset=['last_review']))
listings_raw = listings_raw.dropna(subset=['last_review'])
print(f"Number of rows dropped: {dropped_rows}")
```

```{python}
#| results: hide
#Explanation: Data cleaning validation (Seeded Rows)

listings_raw.sample(10, random_state=42)
```

```{python}
#Explanation: Data cleaning validation (Column Type)

listings_clean = listings_raw
listings_clean.info()
```

```{python}
#Explanation: Save the clean data locally

path = os.path.join('Data','Clean')

if not os.path.exists(path):
    print(f"Creating {path} under {os.getcwd()}") 
    os.makedirs(path)
    
listings_clean.to_csv(os.path.join(path,fn), index=False) 
print("Done.")
```

```{python}
#| results: hide
#Explanation: Loading London Borough boundaries (polygon)

import geopandas as gpd
ddir = os.path.join('data','geo') 
spath = 'https://github.com/jreades/fsds/blob/master/data/src/' 
boros = gpd.read_file(cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )
print('Done!')
boros.to_file('test.gpkg', driver='GPKG')
print(str(boros.geometry.iloc[1])[:50] + "...")
print(boros.crs)
```

```{python}
#| results: hide
#Explanation: Loading Airbnb listings (point)

gdf = gpd.GeoDataFrame(listings_clean,
                       geometry=gpd.points_from_xy(listings_clean.longitude, listings_clean.latitude, crs='epsg:4326'))
print(type(gdf))
print(gdf.geometry.iloc[1])
gdf = gdf.to_crs('epsg:27700') 
print(gdf.geometry.crs) #Reprojection
```

```{python}
#Explanation: Calculate the total number of listings in the cleaned data frame.

num_rows = listings_clean.shape[0]
print(f"Number of rows: {num_rows}")
```

```{python}
#Explanation: Calculate the number of listings for each room type and the proportion of 'Entire home/apt'.

room_type_counts = listings_clean['room_type'].value_counts()

print(room_type_counts)

# Count the total number of listings for each room type
room_type_counts = listings_raw['room_type'].value_counts()

# Calculate the proportion of 'Entire home/apt' relative to the total
entire_home_proportion = room_type_counts['Entire home/apt'] / room_type_counts.sum()

print(f"Proportion of 'Entire home/apt' listings: {entire_home_proportion:.4f}")
```

```{python}
#| results: hide
#Explanation: Calculate the proportion of 'Entire home/apt' hosts as multiple-listing hosts

entire_home_apt = listings_clean[listings_clean['room_type'] == 'Entire home/apt']
more_than_1_entire = entire_home_apt[entire_home_apt['calculated_host_listings_count'] != 1]

entire_total_hosts = len(entire_home_apt['host_name'].unique())
entire_more_than_1_hosts = len(more_than_1_entire['host_name'].unique())
entire_ratio = entire_more_than_1_hosts / entire_total_hosts

print(f"Entire home/apt multiple listing host ratio: {entire_ratio:.2%}")
```

```{python}
#| results: hide
#Explanation: Subsetting the listings (points) to 'Entire home/apt' with 'availability_365' larger than 90.

entire_home_apt_90 = gdf[(gdf['room_type'] == 'Entire home/apt') & 
                          (gdf['availability_365'] > 90) & 
                          (gdf['availability_365'].notna())]
```

As of December 2023, InsideAirbnb reports approximately 66,000 active listings in London, with 43,000 (64.01%) categorized as Entire home/apartment. We can see that most boroughs have a higher proportion of Entire home/apartment listings compared to other room types (Fig 1), with Westminster leading at over 5,000 listings, followed by Kensington and Chelsea, and Camden.

```{python}
#| output: true
#Explanation: Creation of figure showing a ranking of boroughs categorized by room type and ordered from higher to 
#lower values of Entire home listings 

import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

#Reproject to the same crs
boros = boros.to_crs("EPSG:4326") 
gdf = gdf.to_crs("EPSG:4326")

#Perform a spatial join
gdf_with_boros = gpd.sjoin(gdf, boros[['NAME', 'geometry']], how='left', predicate='within')

#Drop NAs
gdf_with_boros = gdf_with_boros.dropna(subset=['NAME'])

#Define categories for map
def categorize_room(row):
    if row['room_type'] == 'Entire home/apt':
        return 'Entire home/apt'
    elif row['room_type'] == 'Private room':
        return 'Private room'
    elif row['room_type'] in ['Shared room', 'Hotel room']:
        return 'Others'
    return 'Other'

#Apply categories
gdf_with_boros['room_category'] = gdf_with_boros.apply(categorize_room, axis=1)

#Count the listings by room category and borough
room_counts = gdf_with_boros.groupby(['NAME', 'room_category']).size().unstack(fill_value=0)

#Only keep the relevant room types
room_counts = room_counts[['Entire home/apt', 'Private room', 'Others']]

#Sort by the "Entire home/apt" column in descending order
room_counts = room_counts.sort_values('Entire home/apt', ascending=True)

#Plot the bar chart
ax = room_counts.plot(kind='barh', stacked=True, figsize=(12, 10), 
                      color=['#ffeb0f', '#ffcf70', '#ffa07a'])

plt.title("Fig 1. Airbnb Listings in London Categorized by Room Type", fontsize=16)
plt.xlabel("Number of Listings", fontsize=12)
plt.ylabel(" ", fontsize=12)
plt.legend(title="Property Type", fontsize=10, loc='lower right', bbox_to_anchor=(1, 0.05))
plt.tight_layout()
x_min, x_max = plt.xlim()  
y_min, y_max = plt.ylim() 

#Add explanation text on legend
plt.text(x_max, y_min, "Others: Sum of Shared room and Hotel room", 
         fontsize=10, color="gray", ha='right', va='bottom')

plt.show()
```

These entire-home listings in London are highly commercialized, with 65.23% available for over 90 days annually and a median availability of 167 days (Fig 2). 

```{python}
#| output: true

#Figure 2 in Matplotlib (for pdf rendering)

availability_365_data = entire_home_apt[entire_home_apt['availability_365'] > 0]['availability_365']

#Get the histogram data
hist, edges = np.histogram(availability_365_data, bins=30)

#Calculate the percentage of data beyond 90
count_beyond_90 = (availability_365_data > 90).sum()
valid_rows = len(availability_365_data)
percentage_beyond_90 = (count_beyond_90 / valid_rows) * 100

#Calculate the median value
median_value = availability_365_data.median()

#Create the Matplotlib figure
fig, ax = plt.subplots(figsize=(10, 6))

#Plot the histogram bars
ax.bar(edges[:-1], hist, width=np.diff(edges), color='orange', alpha=0.7, edgecolor="#4F4F4F", label="Frequency")

#Add a vertical line at 90 days
ax.axvline(x=90, color='black', linestyle='--', linewidth=2, label="90 Days")

#Add a vertical line for the median value
ax.axvline(x=median_value, color='red', linestyle='-.', linewidth=2, label=f"Median Day ({int(median_value)} days)")

#Add annotation for percentage of data beyond 90 days
ax.text(95, max(hist) * 1, f"{percentage_beyond_90:.2f}%\nabove 90 days",
        fontsize=10, color='black', va='top', bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))

#Add annotation for median value
ax.text(median_value + 5, max(hist) * 1, f"Median: {int(median_value)} days",
        fontsize=10, color='red', va='top', bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))

#Customize grid, title, and axis labels
ax.grid(False)
ax.set_title("Fig 2. Distribution of Available Days for Entire Home/Apt in London", fontsize=14)
ax.set_xlabel("Available Days in the Next Calendar Year from December 10, 2023", fontsize=12)
ax.set_ylabel("Frequency", fontsize=12)
ax.legend()

#Adjust y-axis range
ax.set_ylim(0, max(hist) * 1.1)

#Show the figure
plt.tight_layout()
plt.show()
```

```{python}
#Explanation: We decided to also do figure 2 in Bokeh as we consider it would be useful for an html rendering of the file in the future.

from bokeh.models import ColumnDataSource, Label
from bokeh.plotting import figure
from bokeh.io import output_notebook, show

# Assuming entire_home_apt is available in your environment
availability_365_data = entire_home_apt[entire_home_apt['availability_365'] > 0]['availability_365']

# Calculate the histogram data
hist, edges = np.histogram(availability_365_data, bins=30)

# Create a ColumnDataSource for Bokeh
source = ColumnDataSource(data=dict(
    top=hist,
    left=edges[:-1],  # left edge of each bin
    right=edges[1:],  # right edge of each bin
))

# Calculate the percentage of data beyond 90
count_beyond_90 = (availability_365_data > 90).sum()
valid_rows = len(availability_365_data)
percentage_beyond_90 = (count_beyond_90 / valid_rows) * 100

# Calculate the median value
median_value = availability_365_data.median()

# Create the Bokeh figure
p = figure(title="Fig 2. Distribution of Available Days for Entire Home/Apt in London",
           x_axis_label="Available Days in the Next Calendar Year from December 10, 2023",
           y_axis_label="Frequency",
           tools="pan,box_zoom,reset,hover")

# Create the histogram bars (quad creates the bars for histograms)
p.quad(top='top', bottom=0, left='left', right='right', source=source, color='orange', alpha=0.7)

# Add a vertical line at 90 days
p.line([90, 90], [0, max(hist)], line_width=2, color="black", line_dash="dashed", legend_label="90 Days")

# Add a vertical line for the median value
p.line([median_value, median_value], [0, max(hist)], line_width=2, color="red", line_dash="dotdash", legend_label="Median Day")

# Add annotation for percentage of data beyond 90 days
label_90 = Label(x=95, y=max(hist) * 0.95, 
                 text=f'{percentage_beyond_90:.2f}%\nabove 90 days',  
                 text_font_size="12px", text_color="black")
p.add_layout(label_90)

# Add annotation for median data 
median_label = Label(x=median_value + 5, y=max(hist) * 0.95, text=f'Median: {int(median_value)} days',
                     text_font_size="12px", text_color="red")
p.add_layout(median_label)

# Customize grid and axis
p.xgrid.grid_line_color = None
p.y_range.start = 0

# Show the plot
output_notebook()  # Output to Jupyter notebook
show(p)  # Display the plot
```

Also, some hosts manage an exceptionally high number of properties, with a maximum of 2,475 listings, indicating commercial operations rather than traditional hosting. Among entire-home hosts with 0 to 20 listings (Fig 3), only 40% manage a single property.

```{python}
#| output: true

#Figure 3 in maplotlib

multiple_listings = entire_home_apt_90[entire_home_apt_90['host_listings_count'] >= 1]
multiple_listings = multiple_listings[multiple_listings['host_listings_count'] <= 20]

host_counts = multiple_listings['host_listings_count']

#Get the histogram data
bins = np.arange(1, host_counts.max() + 2)  
hist, edges = np.histogram(host_counts, bins=bins, density=True)

#Create the Matplotlib figure
fig, ax = plt.subplots(figsize=(10, 6))

#Plot the histogram bars
ax.bar(edges[:-1], hist, width=np.diff(edges), color='orange', edgecolor="black", alpha=0.7)

#Set the x-axis ticks to match the bins
ax.set_xticks(bins[:-1])

#Add title and axis labels
ax.set_title("Fig 3. Distribution of Listings Owned by Hosts of Entire Home/Apt", fontsize=14)
ax.set_xlabel("Number of Listings per Host", fontsize=12)
ax.set_ylabel("Proportion", fontsize=12)

#Customize grid
ax.grid(True, linestyle='--', alpha=0.5)

#Show the plot
plt.tight_layout()
plt.show()
```

```{python}
#fig3 in bokeh, for html rendering if necessary in the future

from bokeh.plotting import figure, show
from bokeh.models import ColumnDataSource
import numpy as np

multiple_listings = entire_home_apt_90[entire_home_apt_90['host_listings_count'] >= 1]

multiple_listings = multiple_listings[multiple_listings['host_listings_count'] <= 20]

host_counts = multiple_listings['host_listings_count']

bins = np.arange(1, host_counts.max() + 2)  
hist, edges = np.histogram(host_counts, bins=bins, density=True)

source = ColumnDataSource(data={
    'top': hist,
    'left': edges[:-1],
    'right': edges[1:]
})

p = figure(
    title="Fig 3.Distribution of Listings Owned by Hosts of Entire Home/apt",
    x_axis_label="Number of Listings per Host",
    y_axis_label="Proportion",
    width=800,
    height=500
)

p.quad(
    top='top', bottom=0, left='left', right='right', 
    source=source, fill_color="orange", line_color="black", alpha=0.7
)

p.xaxis.ticker = bins[:-1]  
p.grid.grid_line_alpha = 0.5

show(p)
```

```{python}
#Explanation: Calculate the total number of listings in the cleaned data frame.

num_rows = listings_clean.shape[0]
print(f"Number of rows: {num_rows}")

print(boros.columns)
```

These figures suggest that Airbnb in London has shifted from sharing underutilized spaces to a profit-driven model dominated by entire homes, which may negatively impact the local rental market, a topic discussed in the next section.

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?

( 45 points)

```{python}
#Explanation: Perform a spatial join to assign borough information to listings and count entire home/apt listings per borough

#Reproject to British National Grid
entire_home_apt_90 = entire_home_apt_90.to_crs('epsg:27700') 
boros=boros.to_crs('epsg:27700')

#Spatially join listings to boroughs to determine which borough each listing falls within
joined = gpd.sjoin(entire_home_apt_90, boros, how='left', predicate='within')

#Group by 'GSS_CODE' and count the number of points in each group
gss_code_counts = joined.groupby('GSS_CODE').size().reset_index(name='no_entire_home_apt_90')

#Merge the count back to the 'boros' GeoDataFrame
boros = boros.merge(gss_code_counts, on='GSS_CODE', how='left')

#Display the first 5 rows
print(boros.head())  
```

```{python}
#Dependent Variable: Mean monthly rent per bedroom (in entire properties, excluding rooms in shared houses) by borough.

#Define the cache_data function to always re-download the file
def cache_data(url, directory):
    os.makedirs(directory, exist_ok=True)
    file_path = os.path.join(directory, os.path.basename(url))
    response = requests.get(url)
    response.raise_for_status()
    with open(file_path, 'wb') as file:
        file.write(response.content)
    return file_path

#Remote downloading 
url1 = "https://www.ons.gov.uk/file?uri=/economy/inflationandpriceindices/adhocs/2417privaterentalmarketinlondonoctober2023toseptember2024/londonrentalstatsaccessibleq32024.xlsx"
file_path1 = cache_data(url1, os.path.join('Data', 'Regression'))

sheet_names = pd.ExcelFile(file_path1).sheet_names
print("Sheet names:", sheet_names)
sheet_data = pd.read_excel(file_path1, sheet_name='2', skiprows=2) if '2' in sheet_names else pd.read_excel(file_path3, sheet_name=4, skiprows=2)
print(sheet_data.head())

#Load the sheet '2' with skiprows=2
sheet_data = pd.read_excel(file_path1, sheet_name='2', skiprows=2)
sheet_data['Mean'] = pd.to_numeric(sheet_data['Mean'], errors='coerce')

#Loop through each borough to calculate mean_rent
results = []

for borough in sheet_data['Borough'].unique():
    borough_data = sheet_data[sheet_data['Borough'] == borough]
    
#Extract mean values for each category
    studio_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Studio', 'Mean'].dropna().values
    one_bedroom_mean = borough_data.loc[borough_data['Bedroom Category'] == 'One Bedroom', 'Mean'].dropna().values
    two_bedrooms_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Two Bedrooms', 'Mean'].dropna().values / 2
    three_bedrooms_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Three Bedrooms', 'Mean'].dropna().values / 3
    four_or_more_mean = borough_data.loc[borough_data['Bedroom Category'] == 'Four or More Bedrooms', 'Mean'].dropna().values / 4

#Calculate the mean_rent
    mean_rent = (
        (studio_mean[0] if len(studio_mean) > 0 else 0) +
        (one_bedroom_mean[0] if len(one_bedroom_mean) > 0 else 0) +
        (two_bedrooms_mean[0] if len(two_bedrooms_mean) > 0 else 0) +
        (three_bedrooms_mean[0] if len(three_bedrooms_mean) > 0 else 0) +
        (four_or_more_mean[0] if len(four_or_more_mean) > 0 else 0)
    ) / 5

#Append the result
    results.append({
        'Name': borough,
        'mean_rent': mean_rent
    })

boros_rent = pd.DataFrame(results)
print(boros_rent)

#Left join to boros
boros = boros.merge(
    boros_rent,
    left_on='NAME',
    right_on='Name',
    how='left'
)

#Drop the redundant column
boros.drop(columns=['Name'], inplace=True)
print(boros.head())
```

Entire-home listings are often criticized for driving up rents in the LTR market by converting properties into STRs, reducing LTR housing availability. As noted earlier, most Airbnb hosts in London with entire-home listings manage multiple properties and rent them for over 90 days per year, reflecting a shift from optimizing underutilized properties to profit-driven operations that disrupt the LTR market [@duso_airbnb_2024]. To address this, London’s [90-day rule](https://www.london.gov.uk/programmes-strategies/housing-and-land/improving-private-rented-sector/guidance-short-term-and-holiday-lets-london) limits STRs of entire homes to 90 days annually without planning permission. However, less than [5%](https://airbtics.com/airbnb-rules-in-greater-london-united-kingdom/#:~:text=Less%20than%205%25%20of%20Airbnb,space%20for%20short%2Dterm%20rentals.) of Airbnb listings display a registration number, making enforcement difficult for local authorities.

Also, we find that, as of 2024, the average monthly rent for a single room in London is £1,013, which accounts for approximately [**52%**](https://www.thetimes.com/article/graduates-spend-more-than-half-their-wages-on-rent-2d5sxwvv2) of a graduate’s monthly income. The map below (Map 1) illustrates the average monthly rent per bedroom in entire properties (excluding shared accommodations) across London boroughs. **Kensington and Chelsea** has the highest average rent at £1,982 per month, followed by **Westminster** (£1,868) and the **City of London** (£1,460).

```{python}
#| output: true

from matplotlib import cm

#Create a colormap based on the 'mean_rent' 
norm = plt.Normalize(vmin=boros['mean_rent'].min(), vmax=boros['mean_rent'].max())
cmap = cm.get_cmap('YlOrRd', 9)  # Reverse the palette and use 9 colors

#Plot the map
fig, ax = plt.subplots(figsize=(10, 8))

#Plot the geometries of the boroughs with colors based on 'mean_rent'
boros.plot(ax=ax, column='mean_rent', cmap=cmap, linewidth=0.5, edgecolor='#4F4F4F', alpha=1)

#Set the title
ax.set_title("Map 1. Mean Monthly Rent per Bedroom by London Borough (2023)", fontsize=12)

#Hide axes
ax.set_axis_off()

#Add a color bar
sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])  # Empty array, only for color bar
cbar = plt.colorbar(sm, ax=ax, orientation='horizontal', fraction=0.035, pad=0.002)
cbar.set_label('Mean Rent (£)', fontsize=12)

#Plot
plt.tight_layout()
plt.show()
```

```{python}
# Map 1 in bokeh, for html
from bokeh.io import output_notebook, show
from bokeh.models import GeoJSONDataSource, ColorBar, LinearColorMapper, HoverTool, PrintfTickFormatter
from bokeh.plotting import figure
from bokeh.palettes import YlOrRd
import json

merged_json = boros.to_json()

geo_source = GeoJSONDataSource(geojson=merged_json)

color_mapper = LinearColorMapper(palette=YlOrRd[9][::-1], low=boros['mean_rent'].min(), high=boros['mean_rent'].max())

p = figure(
    title="Map 1. Mean Montly Rent per Bedroom by London Borough (2023)",
    width=800,  
    height=600, 
    toolbar_location="right",
    tools="pan,wheel_zoom,reset",
)

p.patches(
    'xs', 'ys', 
    source=geo_source,
    fill_color={'field': 'mean_rent', 'transform': color_mapper},
    line_color="black",
    line_width=0.5,
    fill_alpha=1,
)

hover = HoverTool()
hover.tooltips = [
    ("Borough", "@NAME"),
    ("Mean Rent", "£@mean_rent{0.0}"),
]
p.add_tools(hover)

color_bar = ColorBar(
    color_mapper=color_mapper,
    label_standoff=8,
    width=500,
    height=20,
    location=(0, 0),
    orientation='horizontal',
    formatter=PrintfTickFormatter(format="£%0.0f")  
)
p.add_layout(color_bar, 'below')

p.axis.visible = False
p.grid.grid_line_color = None

output_notebook()
show(p)
```

```{python}
#Explanation: Plot the distribution of mean_rent and log-transform it if it is skewed

# Plot the histogram
plt.figure(figsize=(10, 6))
plt.hist(boros['mean_rent'].dropna(), bins=20, color='skyblue', edgecolor='black')

# Add labels and title
plt.title('Distribution of Mean Rent', fontsize=16)
plt.xlabel('Mean Rent', fontsize=14)
plt.ylabel('Frequency', fontsize=14)

# Show the plot
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

#We see that data is skewed so we applied  Log-transformation
boros['log_mean_rent'] = np.log(boros['mean_rent'] + 1)  # Add 1 to avoid log(0)
print(boros.head())
```

```{python}
#Explanation: Number of households per borough to calculate the independent variable

url2 = "https://data.london.gov.uk/download/housing-tenure-borough/785f6f0e-cc4b-42fd-8093-597b009555f2/tenure-households-borough.xlsx"

#Define function to get files
def cache_data(url, cache_path):
    if not os.path.exists(cache_path):
        os.makedirs(cache_path)
    local_file = os.path.join(cache_path, os.path.basename(url))
    if not os.path.exists(local_file):
        # Download the file if not cached
        import urllib.request
        urllib.request.urlretrieve(url, local_file)
    return local_file

#Extract the sheet and load it
boros_household_raw = pd.read_excel(
    cache_data(url2, os.path.join('Data', 'Regression')),
    sheet_name='2020', 
    header=[0, 1, 2]
)

#Flatten multi-level column names
boros_household_raw.columns = ['_'.join(col).strip() for col in boros_household_raw.columns.values]
print("Flattened columns:", boros_household_raw.columns)

#Select relevant columns (first and seventh)
boros_household = boros_household_raw.iloc[:, [0, 6]]

#Rename columns for clarity
boros_household.columns = ['Unnamed_0', 'no_household']

#Merge boros_household with boros
boros = boros.merge(
    boros_household,
    left_on='GSS_CODE',
    right_on='Unnamed_0',
    how='left'
)

#Drop the redundant column
boros.drop(columns=['Unnamed_0'], inplace=True)
print(boros.head())
```

```{python}
#Key Independent Variable: Number of Entire home/apt with annual availability > 90 per 1000 households by borough & Log-transformation

url2 = "https://data.london.gov.uk/download/housing-tenure-borough/785f6f0e-cc4b-42fd-8093-597b009555f2/tenure-households-borough.xlsx"

#Extract the sheet and load it
boros_household_raw = pd.read_excel(
    cache_data(url2, os.path.join('Data', 'Regression')),
    sheet_name='2020', 
    header=[0, 1, 2]
)

#Flatten multi-level column names
boros_household_raw.columns = ['_'.join(col).strip() for col in boros_household_raw.columns.values]
print("Flattened columns:", boros_household_raw.columns)

#Select relevant columns (first and seventh)
boros_household = boros_household_raw.iloc[:, [0, 6]]

#Rename columns for clarity
boros_household.columns = ['Unnamed_0', 'no_household']

#Merge boros_household with boros
boros = boros.merge(
    boros_household,
    left_on='GSS_CODE',
    right_on='Unnamed_0',
    how='left',
    suffixes=('', '_household')
)

#Drop the redundant column
boros.drop(columns=['Unnamed_0'], inplace=True)
print(boros.head())

#Check and convert columns to numeric
boros['no_entire_home_apt_90'] = pd.to_numeric(boros['no_entire_home_apt_90'], errors='coerce')
boros['no_household'] = pd.to_numeric(boros['no_household'], errors='coerce')

#Create the new column
boros['entire_1000_household'] = (boros['no_entire_home_apt_90'] / boros['no_household']) * 1000
boros['log_entire_1000_household'] = np.log(boros['entire_1000_household'] + 1)

#Display the first few rows to verify
print(boros.head())
```

```{python}
#Explanation: Control 1 - Population Density

url3 = "https://data.london.gov.uk/download/land-area-and-population-density-ward-and-borough/77e9257d-ad9d-47aa-aeed-59a00741f301/housing-density-borough.csv"

boros_population_density = pd.read_csv(cache_data(url3, os.path.join('Data', 'Regression')))

boros_population_density.info(verbose=True)

#Filter boros_population_density for the year 2023 and select relevant columns
boros_population_density_filtered = (
    boros_population_density
    .loc[boros_population_density['Year'] == 2023, 
         ['Code', 'Population', 'Population_per_square_kilometre']]
    .reset_index(drop=True)
)

if 'Code' in boros.columns:
    boros = boros.drop(columns=['Code'])

#Left join
boros = boros.merge(
    boros_population_density_filtered, 
    left_on='GSS_CODE', 
    right_on='Code', 
    how='left', 
    suffixes=('', '_density')
)

#Drop the 'Code' column from the merged DataFrame if it exists
if 'Code' in boros.columns:
    boros = boros.drop(columns=['Code'])

#Display the first few rows of the final DataFrame
print(boros.head())
```

```{python}
#Explanation: Control 2 - Median Taxpayer Income

url4 = "https://data.london.gov.uk/download/average-income-tax-payers-borough/392e86d4-f1d3-4f06-a6a5-7fcd0fd65948/income-of-tax-payers.xlsx"

boros_income_raw = pd.read_excel(cache_data(url4, os.path.join('Data', 'Regression')), 
                                 sheet_name='Total Income', header=[0, 1])


boros_income_code = boros_income_raw[('Unnamed: 0_level_0', 'Code')]

boros_income_median = boros_income_raw[('2021-22', 'Median £')]

boros_income = pd.DataFrame({
    'Code': boros_income_code,
    'Median £': boros_income_median
})

boros_income = boros_income.reset_index(drop=True)

#Perform the left join with the 'boros' DataFrame
boros = boros.merge(boros_income, left_on='GSS_CODE', right_on='Code', how='left')

print(boros.head())
```

```{python}
#Explanation: Control 3 - Unemployment Rate

url5 = "https://www.ethnicity-facts-figures.service.gov.uk/work-pay-and-benefits/unemployment-and-economic-inactivity/unemployment/latest/downloads/unemployment-by-local-authorities.csv"
local_path5 = os.path.join('Data', 'Regression')

unemployment_data_raw = pd.read_csv(cache_data(url5, local_path5))

unemployment_data_filtered = unemployment_data_raw[
    (unemployment_data_raw['time'] == 'Jan2022-Dec2022') &
    (unemployment_data_raw['ethnicity'] == 'All') &
    (unemployment_data_raw['ethnicity_type'] == 'All')
]
unemployment_data_filtered = unemployment_data_filtered[['Geography', 'value']]

# Perform left join
boros = boros.merge(
    unemployment_data_filtered,
    how='left',
    left_on='NAME',
    right_on='Geography'
)
boros.rename(columns={'value': 'unemployment_rate'}, inplace=True)

boros.drop(columns=['Geography'], inplace=True)
print(boros)
```

```{python}
#Explanation: Control 4 - Crime Rate

url6 = "https://data.london.gov.uk/download/recorded_crime_summary/d234c4fe-b060-46bd-a2a6-c5dd1119bddd/MPS%20Borough%20Level%20Crime%20%28most%20recent%2024%20months%29.csv"
local_path6 = os.path.join('Data', 'Regression')

crime_rate_raw = pd.read_csv(cache_data(url6, local_path6))

# Select only columns with '2023'
crime_2023 = crime_rate_raw.filter(like='2023', axis=1)  # Select columns containing '2023'
crime_2023['BoroughName'] = crime_rate_raw['BoroughName']  # Keep the 'BoroughName' column

# Group by 'BoroughName' and calculate the sum
crime_grouped = crime_2023.groupby('BoroughName').sum(numeric_only=True)  # Sum numeric columns
crime_grouped = crime_grouped.reset_index()  # Reset the index to make 'BoroughName' a column
crime_grouped['crime_count'] = crime_grouped.sum(axis=1, numeric_only=True)  # Sum all 2023 columns for each borough

boros_crime = crime_grouped[['BoroughName', 'crime_count']]
print(boros_crime.head())

boros = boros.merge(
    boros_crime,
    how='left',
    left_on='NAME',
    right_on='BoroughName',
    suffixes=('', '_crime')  
)

boros['crime_rate'] = boros['crime_count'] / boros['Population']
print(boros.head())
```

```{python}
#| results: hide
#Explanation: Control 5 - Housing Demand (% of Renters)

url7 = "https://data.london.gov.uk/download/housing-tenure-borough/53cb03c2-3e99-4d71-937b-90b95a64ebaa/tenure-population-borough.csv"
local_path7 = os.path.join('Data', 'Regression')

tenure_data_raw = pd.read_csv(cache_data(url7, local_path7))

tenure_data_raw['Percent_of_population_in_borough'] = pd.to_numeric(
    tenure_data_raw['Percent_of_population_in_borough'], errors='coerce'
)
tenure_filtered = tenure_data_raw[
    (tenure_data_raw['Year'] == 2018) & 
    (tenure_data_raw['Tenure'].isin([
        'Rented.from.Local.Authority.or.Housing.Association',
        'Rented.from.Private.landlord'
    ]))
]

tenure_pivot = tenure_filtered.pivot_table(
    index='Code', 
    columns='Tenure', 
    values='Percent_of_population_in_borough',
    aggfunc='sum'  
).reset_index()

# Renters in both social housing and private housing are considered
tenure_pivot.columns.name = None  
tenure_pivot.rename(columns={
    'Rented.from.Local.Authority.or.Housing.Association': 'percent_local_authority_housing',
    'Rented.from.Private.landlord': 'percent_private_landlord'
}, inplace=True)

# Calculate the sum for the two categories
tenure_pivot['percent_of_renters'] = (
    tenure_pivot['percent_local_authority_housing'] + tenure_pivot['percent_private_landlord']
)

boros_renters = tenure_pivot[['Code', 'percent_local_authority_housing', 'percent_private_landlord', 'percent_of_renters']]
print(boros_renters.head())

boros = boros.merge(
    boros_renters[['Code', 'percent_of_renters']],  
    how='left',  
    left_on='GSS_CODE',  
    right_on='Code'  
)

print(boros.head())
```

```{python}
#Explanation: Control 6 - Housing Supply (Housing stock per person)

url8 = "https://data.london.gov.uk/download/local-authority-housing-stock/3421002e-f0ec-45e5-bf73-f594788ec93d/local-authority-housing-stock-borough.csv"
local_path8 = os.path.join('Data', 'housing-stock-by-local-authorities.csv')

housing_stock_data_raw = pd.read_csv(cache_data(url8, local_path8))
boros_housing_stock = housing_stock_data_raw[['Number of properties 2020', 'Code']].copy()
boros_housing_stock['Number of properties 2020'] = boros_housing_stock['Number of properties 2020'].str.replace(',', '').astype(float)
boros_housing_stock.rename(columns={'Number of properties 2020': 'housing_stock_2020'}, inplace=True)

boros_housing_stock.reset_index(drop=True, inplace=True)
print(boros_housing_stock.head())

boros = boros.merge(
    boros_housing_stock[['Code', 'housing_stock_2020']],  # Select only necessary columns
    how='left',
    left_on='GSS_CODE',
    right_on='Code',
    suffixes=('', '_housing_stock')  # Specify suffixes to avoid column name conflicts
)

if 'Code' in boros.columns:
    boros = boros.drop(columns=['Code'])

boros['number_of_dwellings_per_person'] = boros['housing_stock_2020'] / boros['Population']
print(boros.head())
```

```{python}
#Explanation: Control 7 - Education (Average 8 score)

url9 = "https://data.london.gov.uk/download/gcse-results-by-borough/a8a71d73-cc48-4b30-9eb5-c5f605bc845c/GCSE%20results%20by%20gender.xlsx"
local_path9 = os.path.join('Data', 'Regression')
file_path9 = cache_data(url9, local_path9)

education_data = pd.read_excel(file_path9, sheet_name='2022-23', skiprows=4)
education_data.rename(columns={
    'E09000001': 'Code',
    'Unnamed: 3': 'Average Attainment 8 score per pupil'
}, inplace=True)

boros_education = education_data[['Code', 'Average Attainment 8 score per pupil']].copy()
boros_education.dropna(subset=['Code'], inplace=True)
boros_education.reset_index(drop=True, inplace=True)

print(boros_education.head())

boros = boros.merge(
    boros_education,  
    how='left',  
    left_on='GSS_CODE',  
    right_on='Code'  
)

if 'Code' in boros.columns:
    boros.drop(columns=['Code'], inplace=True)

print(boros.head())
```

```{python}
#Explanation: Control 8 - Public Transport (PTAL) by borough

url10 = "https://data.london.gov.uk/download/public-transport-accessibility-levels/8e520b81-dd06-4ce6-aaa0-972dccf84b57/Borough%20AvPTAI2015.csv"
local_path10 = os.path.join('Data', 'Regression')
file_path10 = cache_data(url10, local_path10)

boros_PTA = pd.read_csv(file_path10)[['Borough Code', 'AvPTAI2015']].copy()
boros = boros.merge(
    boros_PTA,  
    how='left',  
    left_on='GSS_CODE',  
    right_on='Borough Code'  
)

if 'Borough Code' in boros.columns:
    boros.drop(columns=['Borough Code'], inplace=True)

print(boros.head())
```

```{python}
#Explanation: Control 9 - Green Space (% of area as green and blue space) by borough

url11 = "https://data.london.gov.uk/download/green-and-blue-cover/fdff7445-bb87-4584-ada8-d527e7fcec97/green_cover_borough_summary_0.05.xlsx"
local_path11 = os.path.join('Data', 'Regression')
file_path11 = cache_data(url11, local_path11)

boros_green_blue = pd.read_excel(file_path11, sheet_name='borough_green_cover')[['lb_name', 'percent_green+blue']].copy()
boros = boros.merge(
    boros_green_blue,  
    how='left',  
    left_on='GSS_CODE',  
    right_on='lb_name'  
)

if 'lb_name' in boros.columns:
    boros.drop(columns=['lb_name'], inplace=True)
print(boros.head())
```

```{python}
#Explanation: Select only the key columns for regression

print(boros.columns)
key_vars = [
    'NAME', 'GSS_CODE', 'log_mean_rent', 'log_entire_1000_household',
    'Population_per_square_kilometre', 'Median £', 'crime_rate',
    'percent_of_renters', 'number_of_dwellings_per_person', 'AvPTAI2015',
    'percent_green+blue', 'ONS_INNER', 'Population', 'HECTARES', 'geometry'
]

boros = boros[key_vars]
print(boros.head())
print(boros.dtypes)
```

```{python}
#Explanation: Convert Inner/Outer as a numeric binary

boros['ONS_INNER'] = boros['ONS_INNER'].map({'T': 1, 'F': 0})
print(boros.head())
```

```{python}
#| results: hide
#Explanation: Drop 'City of London' and check data frame

boros = boros.drop(index=32)
print(boros)
```

```{python}
#Explanation: OLS Regression

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import pandas as pd
import statsmodels.api as sm

#Define VIF Function
def drop_column_using_vif_(df, thresh=5, protect=[]): 
    while True:
        df_with_const = add_constant(df)
        vif_df = pd.Series(
            [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])],
            name="VIF",
            index=df_with_const.columns
        ).to_frame()
        vif_df = vif_df.drop('const')  # Drop the constant from consideration

        # Find the variable with the highest VIF
        vif_to_drop = vif_df.loc[~vif_df.index.isin(protect)]  # Exclude protected variables
        if vif_to_drop.VIF.max() > thresh:
            index_to_drop = vif_to_drop.index[vif_to_drop.VIF == vif_to_drop.VIF.max()].tolist()[0]
            print(f"Dropping: {index_to_drop}")
            df = df.drop(columns=index_to_drop)
        else: 
            break
    return df

#Select predictors and remove non-numeric columns
predictors = boros.drop(columns=['NAME', 'GSS_CODE', 'HECTARES', 'geometry', 'Population', 'log_mean_rent'])
predictors = predictors.apply(pd.to_numeric, errors='coerce')  

#Run VIF test
df_predictors_selected_VIF = drop_column_using_vif_(predictors, thresh=5, protect=['log_entire_1000_household'])

print("The columns remaining after VIF selection are:")
print(df_predictors_selected_VIF.columns)

#Dependent variable
endog = boros.loc[df_predictors_selected_VIF.index, 'log_mean_rent']

#Independent variables with constant added
exog = sm.add_constant(df_predictors_selected_VIF)

#Perform the OLS regression
OLS_regression_data = sm.OLS(endog=endog, exog=exog).fit()

#Print the regression summary
print(OLS_regression_data.summary())
```

```{python}
#Explanation: Unlog the column (assuming log transformation was natural log: log(x + 1))

boros['entire_1000_household'] = np.exp(boros['log_entire_1000_household']) - 1
print(boros)
```

To understand how much are those listings impacting the LTR market, we analysed Airbnb entire-home listings operating over 90 days a year, using InsideAirbnb’s December 2023 London's dataset. The analysis accounts for borough-level factors such as socio-demographics, LTR demand and supply, and local amenities. Below are these factors with links to their data sources:

**Socio-demographic**: [Population Density](https://data.london.gov.uk/dataset/land-area-and-population-density-ward-and-borough), [Median Income](https://data.london.gov.uk/dataset/average-income-tax-payers-borough), [Unemployment Rate](https://www.ethnicity-facts-figures.service.gov.uk/work-pay-and-benefits/unemployment-and-economic-inactivity/unemployment/latest/#download-the-data), [Crime Rate](https://data.london.gov.uk/dataset/recorded_crime_summary)  

**LTR Demand & Supply**: [Percentage of Renters](https://data.london.gov.uk/dataset/housing-tenure-borough), [Housing Stock per Person](https://data.london.gov.uk/dataset/local-authority-housing-stock)  

**Local Amenities**: [Education](https://data.london.gov.uk/dataset/gcse-results-by-borough), [Public Transport Accessibility](https://data.london.gov.uk/dataset/public-transport-accessibility-levels), [Green Space](https://data.london.gov.uk/dataset/green-and-blue-cover)  

By controlling for these borough-level factors, we find that a **1%** increase in Airbnb entire-home listings operating over 90 days per 1,000 households results in a **0.28%** rise in mean rent. This shows the potential for reducing rents through stricter regulation of such listings in London.

Regarding regulations, we propose policies tailored to London’s short-term entire-home rental market: 

First, since most London hosts [do not display registration numbers](https://airbtics.com/airbnb-rules-in-greater-london-united-kingdom/#:~:text=Less%20than%205%25%20of%20Airbnb,space%20for%20short%2Dterm%20rentals.), making it difficult to identify hosts with multiple properties, we recommend requiring Airbnb to verify that hosts are the legal owners or representatives of their listed properties. This would enhance transparency, allowing regulators to distinguish between individual hosts (1 listing) and commercial hosts (more than 1 listing) and enforce fair, effective regulation. 

Following that, we propose place-based regulations tailored to borough-specific saturation levels of entire-home listings exceeding 90 days per 1,000 households (Map 2). Boroughs are classified by saturation using clustering: red (high), orange (medium), and yellow (low).   

```{python}
#Explanation: Conduct Elbow Test & Clustering

#Prepare the data for clustering
from sklearn.cluster import KMeans
X = boros[['entire_1000_household']].values  # Use the unlogged column for clustering

#Perform the elbow test
wcss = []
for k in range(1, 11):  # Test cluster sizes from 1 to 10
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

#Plot the elbow curve
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.show()
```

```{python}
#| output: true

#Map2
import matplotlib.patches as mpatches

#Perform KMeans clustering
optimal_clusters = 3
X = boros[['entire_1000_household']].values
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
boros['cluster'] = kmeans.fit_predict(X)

#Calculate the average saturation for each cluster
cluster_avg_saturation = boros.groupby('cluster')['entire_1000_household'].mean()

#Sort the clusters based on average saturation in descending order
sorted_clusters = cluster_avg_saturation.sort_values(ascending=False)

#Reassign cluster labels based on the ranking of saturation
cluster_rank = {cluster: rank for rank, cluster in enumerate(sorted_clusters.index)}
boros['cluster_ranked'] = boros['cluster'].map(cluster_rank)

#Define individual colors for each ranked cluster
custom_colors = {0: '#eb6238',  # Custom color for cluster 0 (highest saturation)
                 1: '#eba038',  # Custom color for cluster 1
                 2: '#fff8d6'}  # Custom color for cluster 2 (lowest saturation)

#Plot the map
fig, ax = plt.subplots(figsize=(10, 8))

#Assign colors to each polygon based on 'cluster_ranked'
for cluster, color in custom_colors.items():
    subset = boros[boros['cluster_ranked'] == cluster]
    subset.plot(ax=ax, color=color, edgecolor='#4F4F4F', linewidth=0.5)

#Add descriptions to clusters
cluster_descriptions = {
    0: "High Saturation",
    1: "Medium Saturation",
    2: "Low Saturation"
}

ax.set_title("Map 2: Saturation of Entire Home/Apt Over 90 Days by London Borough (2023)", fontsize=12)

#Hide axis
ax.set_axis_off()


#Add a legend
legend_patches = [
    mpatches.Patch(color=color, label=f"Cluster {rank} - {cluster_descriptions[rank]}")
    for rank, color in custom_colors.items()
]
ax.legend(
    handles=legend_patches,
    loc='lower left',  # Base position
    bbox_to_anchor=(0, -0.02),  # Move the legend downward
    title="Clusters Ranked by Saturation",
    fontsize=10,
    title_fontsize=11,
    ncol=1  # Optional: Controls the number of columns in the legend
)


cbar.ax.yaxis.set_label_position('left')

cbar.set_ticks([0, 1, 2])
cbar.ax.tick_params(labelsize=12)

#Plot
plt.tight_layout()
plt.show()
```

In red and orange boroughs, regulations should minimize disruption for individual hosts relying on Airbnb to earn income from spare rooms or temporarily vacant properties, as discussed in Question 5. For instance, we suggest maintaining the 90-day rental limit and preserving the “Rent a Room” scheme, which allows up to £7,500 in tax-free income, as explained on the [Keynest webpage](https://keynest.com/blog/airbnb-regulations-london). For commercial hosts, we recommend a strict ban to reduce upward pressure on rents in these unaffordable boroughs.   

In yellow boroughs, the same policies for individual hosts as above apply.  For commercial hosts, operations can continue under strict conditions, including adherence to the 90-day rule, obtaining permits, and paying hotel taxes, following similar regulations to San Francisco practices [@Shabrina:2017]. Additionally, we suggest imposing a revenue cap tied to the borough’s average long-term rent to reduce financial incentives for prioritizing STRs over long-term rentals (LTRs). 

These measures, summarized in Figure 4, aim to protect individual hosts using Airbnb responsibly, while applying stricter regulations to commercial operators who exacerbate the housing crisis. Ultimately, these policies seek to ease rising housing costs by reducing homes lost to STRs and restoring them to the LTR market. 

```{python}
#| output: true
from graphviz import Digraph
from IPython.display import SVG, display


# Create a Digraph object
diagram = Digraph(format='svg', graph_attr={'rankdir': 'TB', 'nodesep': '1.2'}, node_attr={'shape': 'box', 'fontsize': '14', 'fontname': 'Helvetica'})

#Add a title node
diagram.node(
    "Title", "Figure 4: Tree Chart of Proposed Regulation", 
    fontsize="18", fontcolor="black", shape="plaintext"
)

#Add nodes and edges
diagram.node(
    "A", "LONDON'S AIRBNB LISTINGS", fontsize="16", penwidth="3.0"
)
diagram.node(
    "B",
    "Host needs to be the real owner \n(landlord/legal occupant)",
    fontsize="14",
    fontcolor="grey",
    color="grey",
)
diagram.node(
    "C",
    "Saturated boroughs",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "D",
    "Individual \n (1 listing)",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "E",
    "Commercial \n (+ 1 listing)",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "F", "Non-saturated boroughs", fontsize="14", penwidth="3.0"
)

diagram.node(
    "H",
    "Individual \n (1 listing)",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "I",
    "Commercial \n (+ 1 listing)",
    fontsize="14",
    penwidth="3.0",
)
diagram.node(
    "J", "Up to 90 days rent \n Up to £7500 tax-free", fontsize="11", fontcolor="grey", color="grey"
)
diagram.node(
    "K",
    "Not allowed",
    fontsize="12",
    fontcolor="grey",
    color="grey",
)
diagram.node("L", "Up to 90 days rent \n Up to £7500 tax-free", fontsize="11", fontcolor="grey", color="grey")

diagram.node(
    "M",
    "Permit and business taxes like hotels \n + \n Revenue cap per each property equivalent\nto the boroughs' average long-term rent",
    fontsize="12",
    fontcolor="grey",
    color="grey",
)

# Connect nodes
diagram.edge("A", "B", dir="none", color="grey")
diagram.edge("B", "C")
diagram.edge("B", "F")
diagram.edge("C", "D", weight="0.5")
diagram.edge("C", "E", weight="0.5")
diagram.edge("F", "H")
diagram.edge("F", "I")
diagram.edge("D", "J", dir="none", color="grey")
diagram.edge("E", "K", dir="none", color="grey")
diagram.edge("H", "L", dir="none", color="grey")
diagram.edge("I", "M", dir="none", color="grey")


# Render and display the diagram in the notebook
svg_data = diagram.pipe(format="svg") 
display(SVG(svg_data))
```

After our analysis, one of the main limitations is the potential inaccuracy of the InsideAirbnb dataset. As mentioned in Question 4, the property type information provided by hosts on the Airbnb platform may not accurately reflect the actual property type, potentially biasing the number of listings included in our analysis. 

Regarding policy implementation, effective enforcement is a pressing issue pending further discussion and proposals, and depends on a smooth agreement between the Greater London Authority and Airbnb for host verification. However, this may be challenging, given that [Airbnb sued San Francisco](https://www.nytimes.com/2017/05/01/technology/airbnb-san-francisco-settle-registration-lawsuit.htmlhttps://www.nytimes.com/2017/05/01/technology/airbnb-san-francisco-settle-registration-lawsuit.html ) in 2016 over a law requiring STR platforms to verify host registration before allowing property listings. 

In conclusion, the InsideAirbnb dataset offers valuable insights into the commercial and unregulated nature of STRs in London, revealing that many entire-home listings exceeding 90 days are owned by hosts with multiple listings. Our analysis found a significant positive relationship between the saturation of such listings and mean rent levels across boroughs. We recommend borough-specific regulations to limit commercial entire-home STRs while protect individual hosts. These measures aim to help alleviate the rent crisis by returning properties to the LTR market. 


## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
